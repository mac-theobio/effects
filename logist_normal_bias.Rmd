Closed form integral of Gaussian over logistic curve?

We are interested in the mean of logistic(gaussian)

Tried it in Python (and Wolfram Alpha). This is just to reiterate what we knew already, which is that the logistic-normal doesn't have a closed form solution (WA can actually do the non-generic case (standard Normal), which sympy can't, but even WA breaks down for non-standard Normal case.

```{python}
from sympy import *
x = Symbol('x')
integrate(exp(-(x**2))/(1+exp(-x)), (x, -oo, oo))
```

A [CrossValidated question](https://stats.stackexchange.com/questions/45267/expected-value-of-a-gaussian-random-variable-transformed-with-a-logistic-functio) about calculating this transformation (and various interesting series approximations etc., although these are very likely not worth the trouble).  Turns out that if we don't want to use the delta method there is an existing package that codes up the mean & variance of the logistic-normal:

```{r mln_ex, eval=FALSE}
logitnorm::momentsLogitnorm(mu= 2, sigma = 2)
##       mean        var 
## 0.77520025 0.06186495 
```

although the entire function definition is not very complicated, we don't really need a package for this ...

```{r mln}
function (mu, sigma, abs.tol = 0, ...) {
    fExp <- function(x) plogis(x) * dnorm(x, mean = mu, sd = sigma)
    .exp <- integrate(fExp, -Inf, Inf, abs.tol = abs.tol, ...)$value
    fVar <- function(x) (plogis(x) - .exp)^2 * dnorm(x, mean = mu, 
        sd = sigma)
    .var <- integrate(fVar, -Inf, Inf, abs.tol = abs.tol, ...)$value
    c(mean = .exp, var = .var)
}
```

Further thoughts about my scribbles: the covariances of the design matrix do almost certainly differ if we condition on values/ranges of a focal predictor, e.g. if vars A and B are strongly correlated and we select a narrow range for A (the focal var), then the variance of B in that slice will inevitably be reduced.  Don't know if there are closed-form solutions for this (even assuming multivariate normality).
