\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx, subfig}
\usepackage{float}
\usepackage{hyperref}

\def\code#1{\texttt{#1}}
\let\proglang=\textsf

\newcommand{\JD}[1]{{\color{blue} \emph{#1}}}
\newcommand{\bmb}[1]{{\color{RubineRed!70!} \emph{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\newcommand{\bX}{{\mathbf X}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\boldeta}{{\boldsymbol \eta}}
\newcommand{\boldmu}{{\boldsymbol \mu}}

\newcommand{\nset}[1]{#1_{\{n\}}}
\newcommand{\yref}{y_{\textrm{ref}}}
\newcommand{\cdist}{{D(\nset{x}|x_f)}}
\newcommand{\cdistprime}{{D(\nset{x}|x_{f'})}}
\newcommand{\xfprime}{x_{f'}}

\newcommand{\yE}{{\mathrm E}}
\let\over=\overline

\setlength{\parindent}{0pt}

\title{Uncertainty propagation for predictor effects and bias correction in generalized linear (mixed) models}

\author{Bicko Cygu, Ben Bolker, Jonathan Dushoff}

\begin{document}

\maketitle
%\setkeys{Gin}{width=0.8\textwidth}


\section{Introduction}

<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(ggpubr)

library(stargazer)

library(vareffects); varefftheme()


set.seed(9991)
load("variable_predictions_objs.rda")
load("justify_preds.rda")
load("cubic_predictors_preds.rda")
load("cubic_predictors_preds_adjust.rda")
load("glm_one_predictor_preds.rda")
load("glme_random_intercept_preds.rda")
load("binom_multiple_outcomes_preds_plots.rda")
@

In many applications, predictions of expected responses, or response probabilities, are often of interest. Predictions provide a great way to summarize what the regression model is telling us and are very useful for interpreting and visualizing model estimates using graphs. For example, in logistic regression models, the coefficient estimates are usually not easy to interpret, and we may want to explore the ``effects" of covariates on predicted probabilities, or predictions of responses at various levels of covariates -- \emph{predictor effects}. For example, as a way to plan, public health officials may want to know the ``effects" of household income, wealth index, etc, on the predicted probability of having improved water services among slum dwellers.

Both simple and generalized linear (mixed) models (GL(M)Ms) can examine very complex relationships, including nonlinear relationships between response and predictors, interactions between predictors (and via splines for example), and nonlinear transformations via link functions due to their flexibility. This flexibility comes at a cost, for example, complex multivariate models may risk misinterpretation, and miscalculation of quantities of interest. Also, coefficient estimates of models involving nonlinear link functions or interactions lose their direct interpretation \citep{leeper2017interpreting}, meaning that interpretation of derived quantities from these estimates requires some understanding of the specified model. 

When visually presented, predictor effects provide a unified and intuitive way of describing relationships from a fitted model, especially complex models involving interaction terms or some kind of transformations on the predictors whose estimates are usually, but not always, a subject to less clarity of interpretation. Further, generating predictor effects together with the associated confidence intervals for regression models has a number of challenges. In particular: 
\begin{enumerate}
\item choice of representative values of \emph{focal} predictor(s) and appropriate \emph{model center} for \emph{non-focal} predictors especially in multivariate models
\item  propagation of uncertainty ([how] can we incorporate uncertainty in nonlinear components of (G)L(M)Ms? Should we exclude variation in non-focal parameters?)
\item bias in the expected mean prediction induced by the nonlinear transformation of the response variable (especially in GL(M)Ms)
\end{enumerate}

Most common way of dealing with the first challenge is taking unique levels of the focal predictor if discrete or taking appropriately sized quantiles (or bins) if continuous, and then calculating the predictions while holding non-focal predictors at their typical values (e.g., averages). This generates -- \emph{predictor effect} \citep{fox2009effect}, \emph{marginal predictions} \citep{leeper2017package} and \emph{estimated marginal means} \citep{lenth2018package}. In this article, we refer to this quantity as \emph{predictor effect} since it should, for example, tell us what we would expect the presponse to be at a particular value or level of the predictor. Formerly, predictor effects computes the expected outcome by meaningfully holding the non-focal predictors constant (or averaged in some meaningful way) while varying the focal predictor, with the goal that the outcome expected prediction represents how the model responds to the changes in the focal predictor.

The go to \proglang{R} packages (\pkg{emmeans} and \pkg{effects}) for generating predictor effects, by default, averages the non-focal predictors. However, there are a number of choices which needs to be made when averaging the non-focal predictors. For example, in the presence of interaction, averaging the interactions (averaging product of interacting predictors) versus product of the averages of the interacting predictors (default for \pkg{emmeans} and \pkg{effects}). We claim that neither of these two approaches is the most appropriate but we show that averaging the interaction closely matches the observed ``truth''. To illustrate this, consider models~\ref{eq:simple_inter_higher_no_interaction} and \ref{eq:simple_inter_higher}  below, with $x_1$ as the focal predictor:
%
\begin{align}
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \epsilon \label{eq:simple_inter_higher_no_interaction}\\
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_{23}x_2x_3 + \epsilon \label{eq:simple_inter_higher}
\end{align}
%
We first simulate data with the two models, such that $x_{1,2,3} \sim \mathrm{Normal}(0, 1)$, $\beta_0 = 5$, $\beta_1 = -0.5$, $\beta_2 = 1$, $\beta_3 = 2$ and $\beta_{23} = 5$, and then compare the predictions from the \pkg{emmeans}, \pkg{effects} and our proposed alternative (\pkg{varpred}) to the known ``truth'' i.e., $\bar{y}$.

In the absence of interaction (model~\ref{eq:simple_inter_higher_no_interaction}), the three approaches have similar estimates, which match the observed ``truth'', Figure~\ref{fig:justify_plots}a. However, in the presence of interaction, even as simple as the one in model~\ref{eq:simple_inter_higher}, the estimates starts to differ. In particular, \pkg{emmeans} and \pkg{effects} give similar estimates ($\bar{\hat{y}}$) but different from the \pkg{varpred}'s which, however, is similar to the observed truth ($\bar{y}$), Figure~\ref{fig:justify_plots}b. To generate Figure~\ref{fig:justify_plots}a, all the three packages averages the non-focal predictors $x_2$ and $x_3$. On other hand, to generate Figure~\ref{fig:justify_plots}b, the difference in the estimates lies on how each of the packages average the interaction term ($x_2x_3$). In particular \pkg{emmeans} and \pkg{effects} computes $\bar{x_2}\bar{x_3}$ while \pkg{varpred} computes $\over{x_2x_3}$.
%
\begin{figure}[H]
\begin{center}
<<justify_plots, echo=FALSE, results=tex, fig=TRUE, message=FALSE>>=
justify_plots
@
\end{center}
\caption{A comparison of predictor effect of $x_1$ on predicted $y$ from \pkg{emmeans}, \pkg{effects} and \pkg{varpred} to the observed. The horizontal blue, green and black lines are the mean of the predictions, i.e., $\bar{\hat{y}}$ while the red one is the mean of the observed $y$, i.e., $\bar{y}$. The trend lines represents the corresponding $\hat{y}$ at various levels of $x_1$, while holding the other predictors at their mean. In the absence of interaction, Figure a, $\bar{\hat{y}}$ closely matches the observed in all the three packages. However, even with the simple interaction between the non-focal predictors, we start seeing larger deviation of $\bar{\hat{y}}$ from $\bar{y}$ for the two commonly used packages (\pkg{emmeans} and \pkg{effects}), but not, the proposed \pkg{varpred}.
}
\label{fig:justify_plots}
\end{figure}
%

An alternative to averaging the non-focal predictors, is the population-based approach which involves computing the prediction over the population of the non-focal predictors and then averaging across the values of the focal predictor. 

In some applications, we may only be interested in the uncertainties associated with the focal predictor -- \emph{isolated} confidence intervals. However, currently, mostly used \proglang{R} packages do not provide straightforward way for \emph{isolating} uncertainties associated with the particular focal predictor. In addition to prediction lines shown in Figure~\ref{fig:justify_plots}a above, we include the confidence bands associated with the predictions for all three methods as shown in Figure~\ref{fig:justify_ci_plots}. For \pkg{emmeans} and \pkg{effects}, the confidence bands are much wider because they include uncertainties associated with the non-focal predictors, but narrower and crosses at the mean of the focal predictor in \pkg{varpred}. In other words, with \pkg{varpred}, we are able to generate isolated confidence bands indicating zero uncertainty at the value of the focal predictor we are more certain about, i.e., mean of the focal predictor. For simple models, the point where the confidence bands crosses is the model center and it corresponds to typical value we choose, in this case, the mean of the focal predictor.
%
\begin{figure}[H]
\begin{center}
<<justify_ci_plots, echo=FALSE, results=tex, fig=TRUE, message=FALSE>>=
justify_ci_plots
@
\end{center}
\caption{A comparison of regular and isolated confidence bands. The description of horizontal and trend lines remain the same as in Figure 1a. The wider dotted blue curves overlaying the green curves are the regular confidence bands for \pkg{emmeans} and \pkg{effects}, while the narrower black curves crossing at the mean of the focal predictor are the isolated confidence bands for \pkg{varpred}. For simple models, the isolated confidence bands crosses at the model center.}
\label{fig:justify_ci_plots}
\end{figure}
%

When dealing with nonlinear link functions, the correct predictions, for example, are even much harder to estimate. One approach is to make predictions on the transformed scale (linear predictor scale), and then back-transform to the original scale. However, the back-transformation may either result in biased predictions or requires some approximation. In particular, bias in expected mean prediction induced by nonlinear transformation of the response variable can lead inaccurate predictions. 

The main purpose of this article is to discuss and implement various approaches for computing predictor effects and provide an alternative method for computing the associated confidence intervals. We further explore approaches for correcting bias in predictions for GL(M)Ms involving nonlinear link functions.


The outline of this article is as follows: Section \ref{sec:qoi} provides formal definition various quantities of interest, Section \ref{sec:stats_background} provides statistical background of estimation of model predictions, Section \ref{sec:uncertainity_propagation} describes mathematical and computation implementation of the proposed method for uncertainty propagation, Section \ref{sec:bias_correction} describes methods for bias correction in GL(M)M together with the computational implementation, and lastly, in Section \ref{sec:simulation_example} we consider some simulation examples.

\section{Quantities of interest}\label{sec:qoi}

Several quantities of interest may be derived from regression models. The first one is the coefficient estimates. Others are \emph{predictors}, \emph{model matrix}, \emph{predicted values} and \emph{marginal effects}. Predictors are model input variables and is associated with one or more \emph{variables} (in polynomials, splines and models with interaction). Model matrix refers to the design matrix whose rows include all combination of variables appearing in the interaction terms, along with the ``typical" values of the focal and non-focal predictors.

To illustrate some of the concepts, consider results from, hypothetical  simulated example, regression of household size as function of household wealth index and age of household head, and the second model involving these predictors plus the interaction between them.

<<qoi_coefs, echo=FALSE, results=tex, message=FALSE, warning=FALSE>>=
stargazer(qoi_mod1, qoi_mod2
	, title = "Example of simple linear model output"
	, single.row=TRUE
	, table.placement="H"
	, label = "tab:qoi_coefs"
	, keep.stat="n"
)
@

In simple linear models with no interaction terms, the default output for the coefficient estimates in Table~\ref{tab:qoi_coefs} model (1) is simple and directly interpretable as the expected change in household size for a unit change in age. In particular, a unit change in age is associated with a household that is \Sexpr{round(coef(qoi_mod1)[[2]], 3)} bigger. This is the \emph{unconditional marginal effect} \citep{leeper2017interpreting} and it is constant across all the observations and levels of all other predictors. As a result, the interpretation of interaction models differs in an important way from linear-additive regression models. To see this more clearly, compare the marginal effect of $x_1$ in the following two models:
%
\begin{align}
y_1 &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon_1 \label{eq:simple_lm}\\
y_2 &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_{12}x_1x_2 + \epsilon_2 \label{eq:simple_inter_lm}
\end{align}
%
The marginal effect of $x_1$ in Equation~\ref{eq:simple_lm} is $\frac{\partial y_1}{\partial x_1} = \beta_1$. On the other hand, the marginal effect of $x_1$ in Equation~\ref{eq:simple_inter_lm} is given by $\frac{\partial y_2}{\partial x_1} = \beta_1 + \beta_{12}x_2$. In other words, if there are no interactions, the marginal effect of $x_1$ on $y_1$ is constant, while, if there are interactions in the model, the marginal effect of a change in $x_1$ on $y_1$ depends on the value of the \emph{conditioning} predictor $x_2$.

Figure~\ref{fig:qoi_pred_plot} shows the comparison of predictor effect and unconditional marginal effect of age on household size, based on model estimates in Table~\ref{tab:qoi_coefs}. Since the model has no interactions, the relationship between the predicted household size and age is linear, hence the marginal effect of age is the slope ($\frac{\Delta \mathrm{hh size}}{\Delta \mathrm{age}}$) of the predictor effect line and can be calculated irrespective of the values of wealth index. 
%
\begin{figure}[H]
\begin{center}
<<qoi_pred_plot, echo=FALSE, results=tex, fig=TRUE, message=FALSE>>=
qoi_age_pred_plot
@
\end{center}
\caption{Predictor effect plot and marginal effect of age of household head on household size. For a linear model with no interaction, the marginal effect is the slope of the predictor effect line.}
\label{fig:qoi_pred_plot}
\end{figure}
%
On the other hand, when model specification involves other kinds of terms such as variable interactions, log and power transformations, etc., the coefficient estimates cannot easily and directly communicate the relationship between the outcome and the independent variable of interest because of the multiple coefficients of the same variable. For instance, in model (2), the interaction between age and wealth index is added, hence the coefficient estimate for age (\Sexpr{round(coef(qoi_mod2)[[2]], 3)}) can only be regarded as unconditional marginal effect when effect of wealth index (and thus \code{age:wealthindex}) is zero, which is never the case. In other words, for models involving interactions terms, looking at a single estimated coefficient in isolation and failing to take into account the estimates involving the interaction terms may lead to inaccurate interpretations \citep{brambor2006understanding, leeper2017interpreting}. 

Predictor effect, on the other hand, is the expected household size for a particular age, holding wealth index at its mean. In particular, the purpose and goal of a predictor effect seems fairly straightforward; for specified values of (a) focal predictor(s), we want to give a point estimate and confidence intervals for the prediction of the model for a ``typical" (= random sample over the multivariate distribution of non-focal parameters) individual with those values of the predictors.




\section{Statistical background}\label{sec:stats_background}

To get an intuition of how conditioning on the mean values of the non-focal predictors work, suppose we are interested in predictor effects of a particular predictor (hence forth referred as \emph{focal} predictor otherwise \emph{non-focal}), $x_f$, from the set of predictors. To keep it simple, assume that the model has no interaction terms. Then the idea is to \emph{anchor} the values of non-focal predictors to some particular values. For example fixing the values of \emph{non-focal} predictor(s) at some typical values -- typically determined by averaging (for now) in some meaningful way, for example, arithmetic mean  for continuous and average over the levels of the factors for categorical non-focal predictors. The easiest way to achieve this is by constructing $\bX^\star$ by averaging the columns of non-focal predictors in model matrix $\bX$, and together with appropriately chosen values of focal predictor(s).


Consider a simple linear model with linear predictor $\eta = \bX\bbeta$ and let $g(\boldmu) = \boldeta$ be an identity link function (in the case of simple linear model), where $\boldmu$ is the expected value of response variable $y$. Let $\hat{\bbeta}$ be the estimate of $\bbeta$, together with the estimated covariance matrix $\Sigma = V(\hat{\bbeta})$ of $\hat{\bbeta}$. Let $\mathbf{X^*}$ be the model matrix, inheriting most of its key properties, for example transformations on predictors and interactions from the model matrix, $\mathbf{X}$. Then the prediction $\hat{\boldeta}^\star = \bX^\star\hat{\bbeta}$ is the predictor effect for the focal predictor in question. 

An alternative, save for later, formulation of predictor effect involves, expressing the linear predictor as the sum of the focal and non-focal predictor linear predictor. In particular, $\eta^\star(x_f, \nset{{\bar{x}}}) = \beta_f x_f + \sum \nset{\beta} \nset{{\bar{x}^\star}}$, where $\nset{{\bar{x}^\star}}$ are the appropriately averaged entries of non-focal predictors and $x_f$ is a vector of values of the focal predictors for a particular observation. 


\subsection{Higher order interactions}

Higher order terms such as interactions, splines, polynomials, etc., can be between the non-focal predictors or focal and non-focal predictor(s). In the former case, we treat the interactions as just another column in the variable space (of the model matrix). In the later case, the non-focal variables in the model matrix are averaged as before. However, for the interacting focal predictors, a combination of each unique levels (or quantiles) are first generated and then the interaction terms are generated by multiplying these combinations. For example, consider model~\ref{eq:simple_inter_higher} above. In the first case, if we consider $x_1$ as the focal predictor, the interaction ($x_2x_3$) is between the non-focal predictors, $x_2$ and $x_3$, the linear predictor is 
%
\begin{align*}
\eta^\star(x_{1i}, \nset{{\bar{x}^\star}}) = \beta_0\mathbf{1} + \beta_1 x_{1i} + \beta_2\bar{x}_2 + \beta_3\bar{x}_3 + \beta_{23}\over{x_2x_3}
\end{align*}
%
where $x_{1i}$ are the carefully chosen levels of the focal predictor. On the other hand, for the second case, if we consider $x_2$ as the focal predictor, then the interaction ($x_2x_3$) is between focal and non-focal predictor. In this case, the linear predictor is given by 
%
\begin{align*}
\eta^\star(x_{2ij}, \nset{{\bar{x}^\star}}) = \beta_0\mathbf{1} + \beta_1 \bar{x}_1 + \beta_2x_{2ij} + \beta_3x_{3j} + \beta_{23}x_{2ij}x_{3j}
\end{align*}
%
In general, our formulation, even for more complicated interactions, follow these two basic principles -- interaction between non-focal predictors and interaction between focal and non-focal predictors.

\section{Uncertainty propagation}\label{sec:uncertainity_propagation}

What about the confidence intervals (CI)? The limits of the confidence intervals are points, not mean values. In principle, every observation/value of focal predictors has a different CI. The traditional way to compute variances for predictions is $\sigma^2 = \textrm{Diag}(\bX^\star \Sigma \bX^{\star\top})$ \citep{lenth2018package, fox2009effect}, so that the confidence intervals are $\eta \pm q\sigma$, where $q$ is an appropriate quantile of Normal or t distribution. This approach incorporates all the uncertainties -- including the uncertainty due to non-focal predictors.  But what if we are only interested in the uncertainty as a result of the focal predictor, so that the confidence intervals are $\eta \pm q \sigma_f$, i.e., \emph{isolated} confidence intervals?  

Currently, commonly used \proglang{R} packages for constructing predictions do not exclude the uncertainties resulting from the non-focal predictors when computing the CIs. A non-trivial way to exclude uncertainties associated with non-focal predictors in some of these packages is to provide a user defined variance-covariance matrix with the covariances of non-focal terms set to $0$ -- \emph{zeroing-out} variance-covariance matrix. This only works when the input predictors are \emph{centered} prior to model fitting, in case of numerical predictors, and even much complicated when the predictors are categorical. We first describe this variance-covariance based approach and then discuss our proposed method which is based on \emph{model centering} and does not require input predictors to be scaled prior to model fitting.


\subsection{Variance-covariance}

The computation of $\hat{\eta}^\star$ remains the same as described above. However, to compute $\sigma$, $\Sigma$ is modified by \emph{zeroing-out} (the variance-covariance of all non-focal predictors are set to zero) variances of non-focal terms. Although this is the simplest approach, it requires centering of continuous, i.e., $x_c = x - \bar{x}$, predictors prior to model fitting and proper way to average categorical predictors.

\subsection{Centered model matrix}

Consider centered model matrix $\bX^{\star}_{c} = \{\bX_f^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$. It follows that the non-focal terms in $\bX^{\star}_{c}$ are all zero in simple models without interactions but are isolated (narrower) around the model center. Consequently the uncertainty due to non-focal predictors are isolated in the computation of $\sigma^2 = \textrm{Diag}(\bX^\star_c \Sigma \bX^{\star\top}_c)$. In addition, the computation of $\bX^{\star}_c$ impacts only on the intercepts and the non-focal terms, i.e., the slopes and variance of the focal predictors are not affected. This means that we can still generate isolated CIs without necessarily centering the predictors prior to model fitting. 


\section{Bias correction}\label{sec:bias_correction}

In many applications, it usually important to report the estimates that reflect the expected values of the untransformed response. However, when dealing with nonlinear link functions, it is even harder to generate correct predictions that reflect the untransformed response due to the bias in the expected mean induced by the nonlinear transformation of the response variable. In such cases, bias correction is needed when back-transforming the predictions to the original scales. Most common approach for bias-adjustment is second-order Taylor approximation \citep{lenth2018package, duursma2003bias}. Here, we describe and implement a different approach, \emph{population-based} approach for bias correction.


\subsection{Population-based approach for bias correction}

The most precise (although not necessarily accurate!) way to predict is to condition on a value $F$ of the focal predictor and make predictions for all observations (members of the population) for which $x_f = F$ (or in a small range around $F$ ...). A key point is that the nonlinear transformation involved in these computations is always \emph{one-dimensional}; all of the multivariate computations required are at the stage of collapsing the multidimensional set of predictors for some subset of the population (e.g. all individuals with $x_f = F$) to a one-dimensional distribution of $\eta^\star$.

Once we have got our vector of $\eta^\star$ (which is essentially a set of samples from the distribution over $\eta^\star(x_f, \nset{x})$ for the conditional set), we may want a mean and confidence intervals on the mean on the response (data) scale, i.e. after back-transforming. In other words we can use the observations themselves then we just compute the individual values of $g^{-1}(\eta^\star)$ and compute their mean.

In population-based approach, $\eta^\star$ is constructed as a function of properly constituted levels of focal predictors, $x_f$, and entire population of the non-focal predictors, $\nset{x}$, as opposed to averaged non-focal predictors, $\nset{{\bar{x}}}$, described in the previous section. More specifically:
%
\begin{itemize}
\item compute linear predictor associated with the non-focal predictors, $\nset{\eta} = \sum \nset{\beta} \nset{x}$
\item compute linear predictor associated with the focal predictors, $\eta_{jf} = \sum{\beta_f x_{jf}}$
\item for every value of the focal predictor, $\eta_{jf}$:
\begin{itemize}
\item $\eta_j^\star  = \eta_{jf} + \nset{\eta}$
\item $\hat{y}_j  = \textrm{mean} ~ g^{-1} \left(\eta_j^\star\right)$
\end{itemize}
\end{itemize}
%
We make similar adjustments to compute the variances of the predictions at every level of the focal predictor:
%
\begin{align}
\sigma_{jf}^2 = \textrm{Diag}(\bX^\star_{jc} \Sigma \bX^{\star\top}_{jc})
\end{align}
%
where $\bX^{\star}_{jc} = \{\bX_{jf}^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$ and 
%
\begin{align}
\mathrm{CI}_j = \mathrm{mean} ~ \eta_j^\star \pm q\sigma_{jf}
\end{align}
%


\section{Simulation examples}\label{sec:simulation_example}


In this section, we illustrate predictor effects (together with the associated CI) in the context of linear models involving continuous and categorical predictors with or without interactions. We compare the expected to the known simulation ``truth".

\subsection{Continuous predictors}\label{sec:continuous_predictors}

Consider simulation example described in Section \ref{sec:qoi}, with slight modifications to incorporate interaction between predictors. The first case, is the model with no interaction: 
%
\begin{align}\label{sim:lm_no_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
Secondly, we add interaction between the non-focal predictors (\code{wealth index} and \code{household expenditure}) but not with the focal predictor \code{age}:
%
\begin{align}\label{sim:lm_nf_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{E}}\mathrm{Expenditure}_i \nonumber\\ 
&+ \beta_{\mathrm{WE}}\mathrm{Wealthindex}_i*\mathrm{Expenditure}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Expenditure}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
\beta_{\mathrm{E}} &= 1.5 \nonumber\\
\beta_{\mathrm{WE}} &= 1 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
And lastly, the model with interaction terms between the focal predictor (\code{age}) and one of the non-focal predictor (\code{wealth index}):
%
\begin{align}\label{sim:lm_f_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{E}}\mathrm{Expenditure}_i \nonumber\\ 
&+ \beta_{\mathrm{AW}}\mathrm{Age}_i*\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Expenditure}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
\beta_{\mathrm{E}} &= 1.5 \nonumber\\
\beta_{\mathrm{AW}} &= 1 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%

We first fitted all the three models and in each case, compared three different predictor effects together with the corresponding CIs -- 1) uncertainty due to non-focal predictors included (everything), 2) uncertainty due to non-focal predictors removed using variance-covariance matrix (isolated (vcov)), and 3) uncertainty due to non-focal predictors removed using centered model matrix (isolated (mm)). The results are shown in Figure~\ref{fig:pred_age_cont_plots}. 

Figure~\ref{fig:pred_age_cont_plots}a represents predictor effects for the model with no interaction, Figure~\ref{fig:pred_age_cont_plots}b shows predictor effects for the model with interaction between non-focal predictors, and Figure~\ref{fig:pred_age_cont_plots}c shows predictor effects for the model with interaction between focal and non-focal predictors. For perfect predictions, we expect the horizontal yellow and grey dotted lines to overlay each other and they should intersect with the black solid line and the vertical dotted grey line at the same point (or very close), i.e., model center. In other words, at the model center (mean of the focal predictor), generally, we expect the model to predict the average of the response variable and narrower CIs. If we properly anchor the model at its center (or any other appropriate value, assuming no interactions), we would expect the variance at the anchor to be zero, i.e., the CIs bands crosses at that point (see isolated (mm) and isolated (vcov) in Figure~~\ref{fig:pred_age_cont_plots}). Although the overall predictor effects trend lines are different in each of the simulation, the expected household size for a household head with average age is not only the same in all the three simulations but also very close to the ``truth''.
%
\begin{figure}[H]
\begin{center}
<<pred_age_cont_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_age_cont_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. Figure a) shows model with no interaction, Figure b) shows model with non-focal predictors interacting with each other, and Figure c) shows model with focal and non-focal predictors interacting. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The dotted black, red (non-focal uncertainty excluded) and the solid blue lines (all uncertainty included) are the predictor effects together with their corresponding CIs. For properly generated predictor effects or simple models with no interactions, we expect the yellow and the grey horizontal dotted lines to overlay each other and intersect with the black, blue, red solid trend lines, and the vertical dotted grey line at the same point (model center).}
\label{fig:pred_age_cont_plots}
\end{figure}
%

\subsection{Polynomial interaction}

We consider more complex interactions in which the focal predictor is modelled as a cubic polynomial. Specifically:
%
\begin{align}\label{sim:lm_cubic}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A_1}}\mathrm{Age}_i + \beta_{\mathrm{A_2}}\mathrm{Age}^2_i + \beta_{\mathrm{A_3}}\mathrm{Age}^3_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 10) \nonumber\\
\beta_0 &= 20 \nonumber\\
\beta_{\mathrm{A}_1} &= 0.1 \nonumber\\
\beta_{\mathrm{A}_2} &= 0.8 \nonumber\\
\beta_{\mathrm{A}_3} &= 0.3 \nonumber\\
\beta_{\mathrm{W}} &= 0.8 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
We may be interested in comparing the observed average household size ($\over{\mathrm{hh~size}}$) and the predicted average household size ($\over{\widehat{\mathrm{hh~size}}}$). 
%
\begin{align}\label{sim:lm_cubic_mean}
\over{\mathrm{hh~size}} &\approx \beta_0 + \beta_{\mathrm{A_1}}\over{\mathrm{Age}_i} + \beta_{\mathrm{A_2}}\over{\mathrm{Age}^2_i} + \beta_{\mathrm{A_3}}\over{\mathrm{Age}^3_i} \nonumber\\ 
	& + \beta_{\mathrm{W}}\over{\mathrm{Wealthindex}_i}
\end{align}
%

We provide analytical comparison in two cases, i.e., when the interaction is in the focal predictor ($\mathrm{Age}$ is the focal predictor)
%
\begin{align}\label{sim:lm_cubic_analytic_age}
\eta^\star(\mathrm{Age}_i, \nset{{\over{\mathrm{Wealthindex}}}}) &= \beta_0 + \beta_{\mathrm{A_1}}\mathrm{Age}_i + \beta_{\mathrm{A_2}}\mathrm{Age}^2_i + \beta_{\mathrm{A_3}}\mathrm{Age}^3_i \nonumber\\ 
	& + \beta_{\mathrm{W}}\over{\mathrm{Wealthindex}} \nonumber\\
\over{{\eta^\star(\mathrm{Age}_i, \nset{{\over{\mathrm{Wealthindex}}}})}} &= \beta_0 + \beta_{\mathrm{A_1}}\over{{\mathrm{Age}_i}} + \beta_{\mathrm{A_2}}\over{{\mathrm{Age}^2_i}} + \beta_{\mathrm{A_3}}\over{{\mathrm{Age}^3_i}} \nonumber\\ 
	& + \beta_{\mathrm{W}}\over{\mathrm{Wealthindex}}
\end{align}
%
and when the interaction is in the non-focal predictor ($\mathrm{Wealthindex}$ is the focal predictor)
%
\begin{align}\label{sim:lm_cubic_analytic_wealthindex}
\eta^\star(\mathrm{Wealthindex}_i, \nset{{\{\over{\mathrm{Age}}, \over{\mathrm{Age}^2}, \over{\mathrm{Age}^2}\}}}) &= \beta_0 + \beta_{\mathrm{A_1}}\over{\mathrm{Age}} + \beta_{\mathrm{A_2}}\over{\mathrm{Age}^2} + \beta_{\mathrm{A_3}}\over{\mathrm{Age}^3} \nonumber\\ 
	& + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i \nonumber\\
\yE(\eta^\star(\mathrm{Wealthindex}_i, \nset{{\{\over{\mathrm{Age}}, \over{\mathrm{Age}^2}, \over{\mathrm{Age}^3}\}}})) &= \beta_0 + \beta_{\mathrm{A_1}}\over{\mathrm{Age}} + \beta_{\mathrm{A_2}}\over{\mathrm{Age}^2} + \beta_{\mathrm{A_3}}\over{\mathrm{Age}^3} \nonumber\\ 
	& + \beta_{\mathrm{W}}\over{\mathrm{Wealthindex}_i}
\end{align}
%
Depending on the choice of the values of the focal predictors in each case, Equation~\ref{sim:lm_cubic_analytic_age} and \ref{sim:lm_cubic_analytic_wealthindex} approximates \ref{sim:lm_cubic_mean} i.e., mean household size at the model center. In Equation~\ref{sim:lm_cubic_analytic_age}, the model center is not a point in the predictor space but rather described by the terms of the cubic polynomial, as opposed to \ref{sim:lm_cubic_analytic_wealthindex} which has the model center as point.

In Figure~\ref{fig:pred_cubic_plots} we compare the non-isolated (everything) and the isolated predictor effects. In the first case (Figure~\ref{fig:pred_cubic_plots}a), the focal predictor, \code{age}, has a higher order interaction in the form of cubic polynomial while in the second case (Figure~\ref{fig:pred_cubic_plots}b), the focal predictor, \code{wealthindex}, is simple but the non-focal predictor is a cubic polynomial. In both cases, the average prediction and the ``truth'' are very similar but in Figure~\ref{fig:pred_cubic_plots}a, cubic polynomial focal predictor, the model center is not a point in the predictor space. On the hand, in Figure~\ref{fig:pred_cubic_plots}b, the model center is a point in the predictor space. 
%
\begin{figure}[H]
\begin{center}
<<pred_cubic_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_cubic_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. In Figure a) the focal predictor has a higher cubic polynomial interaction while in Figure b) the focal predictor has no interaction but the non-focal predictor is a cubic polynomial. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The dotted black (non-focal uncertainty excluded) and blue lines (all uncertainty included) are the predictor effects together with their corresponding CIs.}
\label{fig:pred_cubic_plots}
\end{figure}
%

\subsection{Generalized linear model}

Consider model described in \ref{sim:lm_no_interaction} with only \code{Age} as the predictor such that 
%
\begin{align}\label{sim:glm_one_pred}
\mathrm{logit(status} = 1) &= \eta \nonumber\\
\mathrm{\eta} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \epsilon_i
\end{align}
%
where \code{status} is a binary outcome with 0 for unimproved and 1 for improved water quality in the slums. By having only a single predictor, we may be able to investigate the contribution of nonlinear link function on the predicted probability of improved water quality. 

Figure~\ref{fig:glm_plots} shows the predictor effect of \code{age} on predicted probability of improved water quality. For such a simple generalized linear model, there is no much bias induced by the nonlinear averaging induced by the nonlinear link function. In particular, the predicted average probability of improved water quality, horizontal black line, closely match the observed proportion of households with improved water quality, horizontal solid black line.
%
\begin{figure}[H]
\begin{center}
<<glm_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(glm_plots)
@
\end{center}
\caption{Predictor effect of age on improved water quality. The horizontal black and grey lines are the average predicted probability and observed proportion of improved water quality, respectively. The vertical dotted grey line is the mean age while the red points are the observed bines of proportion of improved water quality.}
\label{fig:glm_plots}
\end{figure}
%

\subsection{Generalized linear mixed effect model}

Here we consider generalized linear models with random effects components. In the simplest case, we start with a single grouping factor as the random effect component, i.e., random intercept model. This way, we can investigate the contribution of random effects overall predicted probability. 

Suppose, in the simulation described in Section~\ref{sec:stats_background}, the observations are  recorded at least once per household and we are interested in the status improved water services (improved or unimproved). In particular, let $\mathrm{H}$ be the number of households indexed by the grouping factor, and $\mathrm{h}[i]$ be the household of the $i$th observation such that 
%
\begin{align}\label{sim:lme_one_pred}
\mathrm{logit(status} = 1) &= \eta \nonumber\\
\mathrm{\eta} &= \beta_0 + \alpha_{0,\mathrm{h}[i]} + \beta_{\mathrm{A}}\mathrm{Age}_i + \epsilon_i \nonumber\\
\alpha_{0,\mathrm{h}} &\sim \mathrm{Normal}(0, 5),~~ h=1, \cdots, \mathrm{10} \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0, 3) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 0.8 \nonumber\\
i &= 1,\cdots, 500
\end{align}
%
We compare predictor effects generated without any bias correction to those corrected for the bias, and then compare the average predicted probability to the observed. The results are shown in Figure~\ref{fig:glme_plots}. The uncorrected approach overestimates the predicted probabilities as opposed to the bias corrected (population-based approach) estimates which closely match the observations and the observed average probability. In the latter approach, the random effects are incorporated (as additional non-focal predictor) in computation of $\nset{\eta}$ to take into account the bias induced by the random effect predictor.
%
\begin{figure}[H]
\begin{center}
<<glme_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(glme_plots)
@
\end{center}
\caption{A comparison of bias corrected and uncorrected predictor effects. The red lines are the bias uncorrected estimates while the black ones are the bias corrected. The horizontal red and black lines are average predicted probability of improved water. The horizontal grey (overlaid by the black) dotted line is the observed average probability of improved water. The vertical dotted grey line is the average age while the grey points are the binned observations.}
\label{fig:glme_plots}
\end{figure}
%

\subsection{Multivariate binary outcomes}

We now consider a bit more complicated model involving two outcome variables (services) -- status (improved or unimproved) of water and garbage services. In particular, we simulate an intercept only multivariate random effect model to, \emph{jointly}, predict probability of improved water and garbage services based on age and wealth index. Each household has its own correlated effect on the intercepts (random-intercept). In addition, we add latent variable to account for unobserved correlations.
%
\begin{align}\label{sim:lme_multi_outcome}
\mathrm{services}_{{\mathrm{h}[i]\mathrm{s}}} &= \mathrm{Binomial}(P_{{\mathrm{h}[i]\mathrm{s}}}) \nonumber\\
\mathrm{logit}(P_{{\mathrm{h}[i]\mathrm{s}}}) &= \beta_{0,s} + \alpha_{0,\mathrm{h}[i]\mathrm{s}} + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{Ls}}\mathrm{Latent}_{i\mathrm{s}} + \epsilon_{i\mathrm{s}} \nonumber\\
\alpha_{0,\mathrm{h}\mathrm{s}} &\sim \mathrm{Normal}(0, 2),~~ h=1, \cdots, \mathrm{H} \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Latent}_{[i]\mathrm{s}} &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_{[i]\mathrm{\text{\tiny{water}}}} &\sim \mathrm{Normal}(0, 2) \nonumber\\
\epsilon_{[i]\mathrm{\text{\tiny{garbage}}}} &\sim \mathrm{Normal}(0, 3) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 2 \nonumber\\
\beta_{\mathrm{W}} &= -3 \nonumber\\
\beta_{\mathrm{L\text{\tiny{water}}}} &= 1 \nonumber\\
\beta_{\mathrm{L\text{\tiny{garbage}}}} &= -5 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
Where:
\begin{itemize}
\item $\beta_{0,s}$ \textbf{is the service specific effect???}, i.e., baseline log odds of a household having improved service
\item $\alpha_{0,\mathrm{h}\mathrm{s}}$ household specific random intercepts, i.e., captures the offset from the overall baseline (mean)
\end{itemize}

Figure~\ref{fig:binom_outcomes_plots} compares bias uncorrected and bias corrected (population-based approach). The bias corrected estimates (black points and curves) closely match the observed proportions (grey points). The top figure shows the expected probability of improved service for an ``average'' household at the baseline. While the two bottom plots shows the predictor effect of age and wealth index on the predicted probability of improved service.
%
\begin{figure}[H]
\begin{center}
<<binom_outcomes_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(binom_outcomes_plots)
@
\end{center}
\caption{Predictor effect plot for multivariate response model. The top plot shows the predicted probability of improved service at the baseline. The bottom plots compares the predictor effect of age and wealth index on the expected probability of improved service. The grey points are the observed proportions (or bins in bottom plots) of improved service. The red points and curves are the bias uncorrected estimates while the black points and curves are the population-based (bias corrected) estimates. Generally, without taking into account the potential bias, we risk overestimating the predicted probabilities.}
\label{fig:binom_outcomes_plots}
\end{figure}
%
 
\clearpage
% References
\bibliographystyle{plainnat}
\bibliography{variable_predictions}

\end{document}
