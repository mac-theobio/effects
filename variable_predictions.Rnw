\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx, subfig}
\usepackage{float}
\usepackage{hyperref}

\def\code#1{\texttt{#1}}
\let\proglang=\textsf

\newcommand{\JD}[1]{{\color{blue} \emph{#1}}}
\newcommand{\bmb}[1]{{\color{RubineRed!70!} \emph{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\newcommand{\bX}{{\mathbf X}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\boldeta}{{\boldsymbol \eta}}
\newcommand{\boldmu}{{\boldsymbol \mu}}

\newcommand{\nset}[1]{#1_{\{n\}}}
\newcommand{\yref}{y_{\textrm{ref}}}
\newcommand{\cdist}{{D(\nset{x}|x_f)}}
\newcommand{\cdistprime}{{D(\nset{x}|x_{f'})}}
\newcommand{\xfprime}{x_{f'}}

\setlength{\parindent}{0pt}

\title{Uncertainty propagation for variable (predictor) effects and bias correction in generalized linear (mixed) models???}

\begin{document}

\maketitle
%\setkeys{Gin}{width=0.8\textwidth}


\section{Introduction}

<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(ggplot2)
library(ggpubr)

library(stargazer)

library(vareffects); varefftheme()


set.seed(9991)
load("variable_predictions_objs.rda")
load("categorical_predictors.rda")
load("lme_random_intercept.rda")
load("multiple_outcomes_preds.rda")
load("cubic_predictors_preds.rda")
@

In many applications, predictions of expected responses, or response probabilities, are often of interest. Predictions provide a great way to summarize what the regression model is telling us and are very useful for interpreting and visualizing models estimates using graphs. For example, in logistic regression models, the coefficient estimates are usually not easy to interpret, and we may want to explore the ``effects" of covariates on predicted probabilities, or predictions of responses at various levels of covariates -- \emph{predictor effects}. For example, as a way to plan, public health officials may want to know the ``effects" of household income, wealth index, etc, on the predicted probability of having improved water services among slum dwellers.

Both simple and generalized linear (mixed) models (GL(M)Ms) can examine very complex relationships, including nonlinear relationships between response and predictors, interactions between predictors (and via splines for example), and nonlinear transformations via link functions due to their flexibility. This flexibility comes at a cost, for example, complex multivariate models may risk misinterpretation, and miscalculation of quantities of interest. Also, coefficient estimates of models involving nonlinear link functions or interactions lose their direct interpretation \citep{leeper2017interpreting}, meaning that interpretation of derived quantities from these estimates requires some understanding of the specified model. 

When visually presented, predictor effects provide a unified and intuitive way of describing relationships from a fitted model, especially complex models involving interaction terms or some kind of transformations on the predictors whose estimates are usually, but not always, a subject to less clarity of interpretation. Further, generating predictor effects together with the associated confidence intervals for regression models has a number of challenges. In particular: 
\begin{enumerate}
\item choice of representative values of \emph{focal} predictor(s) and appropriate ``model center" for \emph{non-focal} predictors especially in multivariate models
\item  propagation of uncertainty ([how] can we incorporate uncertainty in nonlinear components of (G)LMMs? Should we exclude variation in non-focal parameters?)
\item bias in the expected mean prediction induced by the nonlinear transformation of the response variable (especially in GL(M)M)
\end{enumerate}

Most common way of dealing with the first challenge is taking unique levels of the focal predictor if discrete or taking appropriately sized quantiles (or bins) if continuous, and then calculating the predictions while holding non-focal predictors at their typical values (e.g., averages). This generates -- \emph{predictor effect} \citep{fox2009effect}, \emph{marginal predictions} \citep{leeper2017package} and \emph{estimated marginal means} \citep{lenth2018package}. In this article, we refer to this quantity as \emph{predictor effect} since it should, for example, tell us what we would expect the presponse to be at a particular value or level of the predictor. An alternative to averaging the non-focal predictors, is the population-based approach which involves computing the prediction over the population of the non-focal predictors and then averaging across the values of the focal predictor. Dealing with second challenge is discussed in Section~\ref{sec:uncertainity_propagation}

When dealing with nonlinear link functions, the correct predictions for example, are even much harder to estimate. One approach is to make predictions on the transformed scale (linear predictor scale), and then back-transform to the original scale. However, the back-transformation may either result in biased predictions or requires some approximation. In particular, bias in expected mean prediction induced by nonlinear transformation of the response variable can lead inaccurate predictions. 

The main purpose of this article is to discuss and implement various approaches for computing predictor effects and provide an alternative method for computing the associated confidence intervals. We further explore approaches for correcting bias in predictions for GL(M)Ms involving nonlinear link functions.

The outline of this article is as follows: Section \ref{sec:qoi} provides formal definition various quantities of interest, Section \ref{sec:stats_background} provides statistical background of estimation of model predictions, Section \ref{sec:uncertainity_propagation} describes mathematical and computation implementation of the proposed method for uncertainty propagation, and lastly Section \ref{sec:bias_correction} describes methods for bias correction in GL(M)M together with the computational implementation.

\section{Quantities of interest}\label{sec:qoi}

Several quantities of interest may be derived from regression models. The first one is the coefficient estimates. Others are \emph{predictors}, \emph{model matrix}, \emph{predicted values} and \emph{marginal effects}. Predictors are model input variables and is associated with one or more \emph{variables} (in polynomials, splines and models with interaction). Model matrix refers to the design matrix whose rows include all combination of variables appearing in the interaction terms, along with the ``typical" values of the focal and non-focal predictors.

To illustrate some of the concepts, consider results from, hypothetical  simulated example, regression of household size as function of household wealth index and age of household head, and the second model involving these predictors plus the interaction between them.

<<qoi_coefs, echo=FALSE, results=tex, message=FALSE, warning=FALSE>>=
stargazer(qoi_mod1, qoi_mod2
	, title = "Example of simple linear model output"
	, single.row=TRUE
	, table.placement="H"
	, label = "tab:qoi_coefs"
	, keep.stat="n"
)
@

In simple linear models with no interaction terms, the default out for the coefficient estimates in Table~\ref{tab:qoi_coefs} model (1) is simple and directly interpretable as the expected change in household size for a unit change in age. In particular, a unit change in age is associated with a household that is \Sexpr{round(coef(qoi_mod1)[[2]], 3)} bigger. This is the \emph{unconditional marginal effect} \citep{leeper2017interpreting} and it is constant across all the observations and levels of all other predictors. As a result, the interpretation of interaction models differs in an important way from linear-additive regression models. To see this more clearly, compare the marginal effect of $x_1$ in the following two models:
%
\begin{align}
y_1 &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon_1 \label{eq:simple_lm}\\
y_2 &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_{12}x_1x_2 + \epsilon_2 \label{eq:simple_inter_lm}
\end{align}
%
The marginal effect of $x_1$ in Equation~\ref{eq:simple_lm} is $\frac{\partial y_1}{\partial x_1} = \beta_1$. On the other hand, the marginal effect of $x_1$ in Equation~\ref{eq:simple_inter_lm} is given by $\frac{\partial y_2}{\partial x_1} = \beta_1 + \beta_{12}x_2$. In other words, if there are no interactions, the marginal effect of $x_1$ on $y_1$ is constant, while, if there are interactions in the model, the marginal effect of a change in $x_1$ on $y_1$ depends on the value of the \emph{conditioning} predictor $x_2$.

Figure~\ref{fig:qoi_pred_plot} shows the comparison of predictor effect and unconditional marginal effect of age on household size, based on model estimates in Table~\ref{tab:qoi_coefs}. Since the model has no interactions, the relationship between the predicted household size and age is linear, hence the marginal effect of age is the slope ($\frac{\Delta \mathrm{hh size}}{\Delta \mathrm{age}}$) of the predictor effect line and can be calculated irrespective of the values of wealth index. 
%
\begin{figure}[H]
\begin{center}
<<qoi_pred_plot, echo=FALSE, results=tex, fig=TRUE, message=FALSE>>=
qoi_age_pred_plot
@
\end{center}
\caption{Variable effect plot and marginal effect of age of household head on household size. For a linear model with no interaction, the marginal effect is the slope of the predictor effect line.}
\label{fig:qoi_pred_plot}
\end{figure}
%
Predictor effect, on the other hand, is the expected household size for a particular age, holding wealth index at its mean. In particular, the purpose and goal of a predictor effect seems fairly straightforward; for specified values of (a) focal predictor(s), we want to give a point estimate and confidence intervals for the prediction of the model for a ``typical" (= random sample over the multivariate distribution of non-focal parameters) individual with those values of the predictors.

On the other hand, when model specification involves other kinds of terms such as variable interactions, log and power transformations, etc., the coefficient estimates cannot easily and directly communicate the relationship between the outcome and the independent variable of interest because of the multiple coefficients of the same variable. For instance, in model (2), the interaction between age and wealth index is added, hence the coefficient estimate for age (\Sexpr{round(coef(qoi_mod2)[[2]], 3)}) can only be regarded as unconditional marginal effect when effect of wealth index (and thus \code{age:wealthindex}) is zero, which is never the case. In other words, for models involving interactions terms, looking at a single estimated coefficient in isolation and failing to take into account the estimates involving the interaction terms may lead to inaccurate interpretations \citep{brambor2006understanding, leeper2017interpreting}. 



\section{Statistical background}\label{sec:stats_background}

To get an intuition of how conditioning on the mean values of the non-focal predictors work, suppose we are interested in predictor effects of a particular predictor (hence forth referred as \emph{focal} predictor otherwise \emph{non-focal}), $x_f$, from the set of predictors. To keep it simple, assume that the model has no interaction terms. Then the idea is to \emph{anchor} the values of non-focal predictors to some particular values. For example fixing the values of \emph{non-focal} predictor(s) at some typical values -- typically determined by averaging (for now) in some meaningful way, for example, arithmetic mean  for continuous and average over the levels of the factors for categorical non-focal predictors. The easiest way to achieve this is by constructing $\bX^\star$ by averaging the columns of non-focal predictors in model matrix $\bX$, and together with appropriately chosen values of focal predictor(s).


Consider a simple linear model with linear predictor $\eta = \bX\bbeta$ and let $g(\boldmu) = \boldeta$ be an identity link function (in the case of simple linear model), where $\boldmu$ is the expected value of response variable $y$. Let $\hat{\bbeta}$ be the estimate of $\bbeta$, together with the estimated covariance matrix $\Sigma = V(\hat{\bbeta})$ of $\hat{\bbeta}$. Let $\mathbf{X^*}$ be the model matrix, inheriting most of its key properties, for example transformations on predictors and interactions from the model matrix, $\mathbf{X}$. Then the prediction $\hat{\boldeta}^\star = \bX^\star\hat{\bbeta}$ is the predictor effect for the focal predictor in question. 

An alternative, save for later, formulation of predictor effect involves, expressing the linear predictor as the sum of the focal and non-focal predictor linear predictor. In particular, $\eta^\star(x_f, \nset{{\bar{x}}}) = \beta_f x_f + \sum \nset{\beta} \nset{{\bar{x}^\star}}$, where $\nset{{\bar{x}^\star}}$ are the appropriately averaged entries of non-focal predictors and $x_f$ is a vector of values of the focal predictors for a particular observation. 

Generally, in generalized linear models with other link functions, the predictions are first generated in linear predictor scale, $\hat{\boldeta}^*$, and then transformed to the original response scale using $g^{-1}(\hat{\boldeta}^*)$.



\section{Uncertainty propagation}\label{sec:uncertainity_propagation}

What about the confidence intervals (CI)? The limits of the confidence intervals are points, not mean values. In principle, every observation/value of focal predictors has a different CI. The traditional way to compute variances for predictions is $\sigma^2 = \textrm{Diag}(\bX^\star \Sigma \bX^{\star\top})$ \citep{lenth2018package, fox2009effect}, so that the confidence intervals are $\eta \pm q\sigma$, where $q$ is an appropriate quantile of Normal or t distribution. This approach incorporates all the uncertainties -- including the uncertainty due to non-focal predictors.  But what if we are only interested in the uncertainty as a result of the focal predictor, so that the confidence intervals are $\eta \pm q \sigma_f$. 

Currently, commonly used \proglang{R} packages for constructing predictions do not exclude the uncertainties resulting from the non-focal predictors when computing the CIs. A non-trivial way to exclude uncertainties associated with non-focal predictors in some of these packages is to provide a user defined variance-covariance matrix with the covariances of non-focal terms set to $0$ -- \emph{zeroing-out} variance-covariance matrix. This only works when the input predictors are \emph{centered} prior to model fitting, in case of numerical predictors, and even much complicated when the predictors are categorical. We first describe this variance-covariance based approach and then discuss our proposed method which is based on \emph{model centering} and does not require input predictors to be scaled prior to model fitting. Model center is found by averaging the variables over the model matrix, $\bX^\star$.


\subsection{Variance-covariance}

The computation of $\hat{\eta}^\star$ remains the same as described above. However, to compute $\sigma$, $\Sigma$ is modified by \emph{zeroing-out} (the variance-covariance of all non-focal predictors are assigned zero) variances of non-focal terms. Although this is the simplest approach, it requires centering of continuous predictors prior to model fitting and proper way to average categorical predictors. One way to center predictors is to create a design matrix, $\bX$, and then compute $\bX_{c} = \bX - \bar{\bX}$.

\subsection{Centered model matrix}

Consider centered model matrix $\bX^{\star}_{c} = \{\bX_f^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$. It follows that the non-focal terms in $\bX^{\star}_{c}$ are all zero. Consequently the uncertainty due to non-focal predictors are all zeroed-out in the computation of $\sigma$. More generally, centered design matrix, $\bX^{\star}_{c}$, impacts on the estimated value of the intercept and its associated variance but not the slopes. Thus since non-focal terms in $\bX^{\star}_{c}$ are all zero, it does not matter what their corresponding values are in the variance-covariance matrix. Hence, we can compute CIs from non-centered predictors (in other words, fitted models with predictors in their natural scales).

\subsection{Handling higher order terms}

Higher order terms such as interactions, splines, polynomials, etc., can be between the non-focal predictors or focal and non-focal predictor(s). In the former case, we treat the interactions as just another column in the variable space (of the model matrix). \textbf{Still figuring out the later case but currently treating everything as just additional column???}


\subsection{Simulation examples}


In this section, we illustrate predictor effects (together with the associated CI) in the context of linear models involving continuous and categorical predictors with or without interactions. We compare the expected to the known simulation ``truth".

\subsubsection{Continuous predictors}\label{sec:continuous_predictors}

Consider simulation example described in Section \ref{sec:qoi}, with slight modifications to incorporate interaction between predictors. The first case, is the model with no interaction: 
%
\begin{align}\label{sim:lm_no_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
Secondly, we add interaction between the non-focal predictors (\code{wealth index} and \code{household expenditure}) but not with the focal predictor \code{age}:
%
\begin{align}\label{sim:lm_nf_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{E}}\mathrm{Expenditure}_i \nonumber\\ 
&+ \beta_{\mathrm{WE}}\mathrm{Wealthindex}_i*\mathrm{Expenditure}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Expenditure}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
\beta_{\mathrm{E}} &= 1.5 \nonumber\\
\beta_{\mathrm{WE}} &= 1 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
And lastly, the model with interaction terms between the focal predictor (\code{age}) and one of the non-focal predictor (\code{wealth index}):
%
\begin{align}\label{sim:lm_f_interaction}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{E}}\mathrm{Expenditure}_i \nonumber\\ 
&+ \beta_{\mathrm{AW}}\mathrm{Age}_i*\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Expenditure}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
\beta_{\mathrm{E}} &= 1.5 \nonumber\\
\beta_{\mathrm{AW}} &= 1 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%

We first fitted all the three models and in each case, compared three different predictor effects together with the corresponding CIs -- 1) uncertainty due to non-focal predictors included (everything), 2) uncertainty due to non-focal predictors removed using variance-covariance matrix (zero-vcov), and 3) uncertainty due to non-focal predictors removed using centered model matrix (centered mm). The results are shown in Figure~\ref{fig:pred_age_cont_plots}. 

Figure~\ref{fig:pred_age_cont_plots}a represents predictor effects for the model with no interaction, Figure~\ref{fig:pred_age_cont_plots}b shows predictor effects for the model with interaction between non-focal predictors, and Figure~\ref{fig:pred_age_cont_plots}c shows predictor effects for the model with interaction between focal and non-focal predictors. For perfect predictions, we expect the yellow and grey dotted lines to overlay each other and they should intersect with the black solid line and the vertical dotted grey line at the same point (or very close). In other words, at the model center (mean of the focal predictor), generally, we expect the model to predict the average of the response variable and narrower CIs. If we properly anchor the model at its center (or any other appropriate value, assuming no interactions), we would expect the variance at the anchor to be zero, i.e., the CIs bands crosses at that point (see zero-cov and centered mm in Figure~~\ref{fig:pred_age_cont_plots}). Although the overall predictor effects trend lines are different in each of the simulation, the expected household size for a household head with average age is not only the same in all the three simulations but also very close to the ``truth''.
%
\begin{figure}[H]
\begin{center}
<<pred_age_cont_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_age_cont_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. Figure a) shows model with no interaction, Figure b) shows model with non-focal predictors interacting with each other, and Figure c) shows model with focal and non-focal predictors interacting. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The solid black, red (non-focal uncertainty excluded) and blue lines (all uncertainty included) are the predictor effects while their corresponding CIs are the dotted black, red and blue lines, respectively. For properly generated predictor effects, we expect the yellow and the grey horizontal dotted lines to overlay each other and intersect with the black, blue, red solid trend lines, and the vertical dotted grey line at the same point (model center).}
\label{fig:pred_age_cont_plots}
\end{figure}
%

\subsection{Polynomial interaction}

We consider more complex interactions in which the focal predictor is modelled as a cubic polynomial. Specifically:
%
\begin{align}\label{sim:lm_cubic}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}^3_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Uniform}(-1, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.3 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%


%
\begin{figure}[H]
\begin{center}
<<pred_cubic_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_cubic_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. Figure a) shows model with no interaction, Figure b) shows model with non-focal predictors interacting with each other, and Figure c) shows model with focal and non-focal predictors interacting. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The solid black, red (non-focal uncertainty excluded) and blue lines (all uncertainty included) are the predictor effects while their corresponding CIs are the dotted black, red and blue lines, respectively. For properly generated predictor effects, we expect the yellow and the grey horizontal dotted lines to overlay each other and intersect with the black, blue, red solid trend lines, and the vertical dotted grey line at the same point (model center).}
\label{fig:pred_cubic_plots}
\end{figure}
%


\subsubsection{Categorical predictors}\label{sec:categorical_predictors}

Suppose, in Section~\ref{sec:continuous_predictors}, in addition to the predictors already defined, we add gender of the household head. For example, we may want to know whether men (M) and females (F) differ in the relationship between age of the household head and the size of the household -- how does households with male as their heads differ in size as compare to those with female heads? 

The first model in this case, slightly modifies the first model in Section~\ref{sec:continuous_predictors} by adding gender of the household head. The proportion of households with males as their household heads is $60\%$:
%
\begin{align}\label{sim:lm_no_interaction_cat}
\mathrm{hh~size} &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{M}}{\mathrm{Gender}_i} + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Gender}_i &\sim \mathrm{Sample}(\mathrm{Gender_i}, p) \nonumber\\
p &= \begin{cases}
   0.4 & \mathrm{Gender_{i=Female}} \nonumber\\
   0.6 & \mathrm{Gender_{i=Male}} \nonumber\\
  \end{cases} \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.8 \nonumber\\
\beta_{\mathrm{M}} &= 0.2 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
The second model has interaction between non-focal predictors (\code{age} and \code{wealth index}), while the third model has interaction between focal (\code{gender} and \code{age}). The results are shown in Figure~\ref{fig:pred_gender_cat_plots}, which compares the predictors effects (either centered or all uncertainty included -- everything) for no interaction model (a), interaction between non-focal predictors (b) and interaction between focal and non-focal predictor (c). For each gender, we expect the observed average household size (``truth" -- red dots), to be very close to the estimated predictor effects (blue and black dots).
%
\begin{figure}[H]
\begin{center}
<<pred_gender_cat_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_gender_cat_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. Figure a) shows model with no interaction, Figure b) shows model with non-focal predictors interacting with each other, and Figure c) shows model with focal and non-focal predictors interacting. The red dots are the observed observed/true means. The black (non-focal uncertainty excluded) and blue solid dots (all uncertainty included) are the predictor effects with their corresponding CIs, respectively. As espected, the observed average household size (red) is very similar to the predicted (red and blue).} 
\label{fig:pred_gender_cat_plots}
\end{figure}
%

\subsubsection{Linear mixed effect model}

We now consider linear models with additional random effects components. In the simplest case, we start with a single grouping factor as the random effect component, i.e., random intercept model. This way, we can investigate the contribution of random effects overall expected mean. 

Suppose, in the simulation described in Section~\ref{sec:stats_background}, the observations are  recorded at least once per household. In particular, let $\mathrm{H}$ be the number of households indexed by the grouping factor, and $\mathrm{h}[i]$ be the household of the $i$th observation. We simulate three different models incorporating this kind of formulation -- first, one grouping factor-one predictor-linear random effect model:
%
\begin{align}\label{sim:lme_one_pred}
\mathrm{hh~size} &= \beta_0 + \alpha_{0,\mathrm{h}[i]} + \beta_{\mathrm{A}}\mathrm{Age}_i + \epsilon_i \nonumber\\
\alpha_{0,\mathrm{h}} &\sim \mathrm{Normal}(0, 1),~~ h=1, \cdots, \mathrm{10} \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 2) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
i &= 1,\cdots, 50
\end{align}
%
The second and third models add the random effects, $\alpha_{\mathrm{h}[i]}$, to models~\ref{sim:lm_nf_interaction} and~\ref{sim:lm_f_interaction}, respectively. Figre~\ref{fig:pred_age_cont_lme_plots} shows and compares predictor efffects for the three models.
%
\begin{figure}[H]
\begin{center}
<<pred_age_cont_lme_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_age_cont_lme_plots)
@
\end{center}
\caption{A comparison of predictor effects and their corresponding CIs. Figure a) shows model with no interaction, Figure b) shows model with non-focal predictors interacting with each other, and Figure c) shows model with focal and non-focal predictors interacting. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The solid black, red (non-focal uncertainty excluded) and blue lines (all uncertainty included) are the predictor effects while their corresponding CIs are the dotted black, red and blue lines, respectively. For properly generated predictor effects, we expect the yellow and the grey horizontal dotted lines to overlay each other and intersect with the black, blue, red solid trend lines, and the vertical dotted grey line at the same point (model center).}
\label{fig:pred_age_cont_lme_plots}
\end{figure}
%


\subsubsection{Multiple outcomes}

We now consider a bit more complicated model involving more two outcome variables (services) -- household size and rental price. In particular, we simulate an intercept only multivariate random effect model to, \emph{jointly}, predict household size and rental price based on age and wealth index. Each household has its own correlated effect on the intercepts (random-intercept). In addition, we add latent variable to account for unobserved correlations.
%
\begin{align}\label{sim:lme_multi_outcome}
\mathrm{y}_{\mathrm{h}[i]\mathrm{s}} &= \beta_{0,s} + \alpha_{0,\mathrm{h}[i]\mathrm{s}} + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \beta_{\mathrm{Ls}}\mathrm{Latent}_{i\mathrm{s}} + \epsilon_{i\mathrm{s}} \nonumber\\
\alpha_{0,\mathrm{h}\mathrm{s}} &\sim \mathrm{Normal}(0, 2),~~ h=1, \cdots, \mathrm{H} \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0.2, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Latent}_{i\mathrm{s}} &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_{i\mathrm{s}} &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 1.5 \nonumber\\
\beta_{\mathrm{A}} &= 0.1 \nonumber\\
\beta_{\mathrm{W}} &= 2 \nonumber\\
\beta_{\mathrm{L}1} &= 1 \nonumber\\
\beta_{\mathrm{L}2} &= -5 \nonumber\\
i &= 1,\cdots, 100
\end{align}
%
Where:
\begin{itemize}
\item $\beta_{0,s}$ \textbf{is the service specific effect???}, i.e., baseline household size and rental price
\item $\alpha_{0,\mathrm{h}\mathrm{s}}$ household specific random intercepts, i.e., captures the offset from the overall baseline (mean)
\end{itemize}

Predictor effects for the above model are show in Figure~\ref{fig:pred_cont_joint_plots}. The top plot compares centered and model centered predictor effects with the observed average household size and rental price. The point estimates (black and blue dots) are the expected baseline household size and the rental price. The two bottom plots compares the predictor effect (of age) on the predicted household size and rental price. Again, in the case of joint model, at the model center, the expected predictions are very similar to the observed marginal averages for the outcomes. 
%
\begin{figure}[H]
\begin{center}
<<pred_cont_joint_plots, echo=FALSE, fig=TRUE, message=FALSE>>=
print(pred_cont_joint_plots)
@
\end{center}
\caption{Predictor effect of age on the predicted household size and rental price. The top plots shows the baseline (mean) predicted household size and rental price. The bottom plots compares the predictor effect of age on the predicted household size and rental price. The dotted yellow and grey horizontal lines are the expected and observed/true means, respectively. The vertical grey line is the mean of the focal predictor (model center). The solid black, red (non-focal uncertainty excluded) and blue lines (all uncertainty included) are the predictor effects while their corresponding CIs are the dotted black, red and blue lines, respectively. For properly generated predictor effects, we expect the yellow and the grey horizontal dotted lines to overlay each other and intersect with the black, blue, red solid trend lines, and the vertical dotted grey line at the same point (model center). Without removing the variations due to non-focal predictors, the confidence bands are very wide ("everything" in the figure). However, when CIs are centered, we are only focussing on the uncertainty due to the focal predictor.}
\label{fig:pred_cont_joint_plots}
\end{figure}
%



\section{Bias correction}\label{sec:bias_correction}

\clearpage
% References
\bibliographystyle{plainnat}
\bibliography{variable_predictions}

\end{document}
