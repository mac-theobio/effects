---
title: "Bias correction in GLMs"
author: Bicko, Jonathan & Ben
date: "2021 Jun 21 (Mon)"
compact-title: false
output:
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.pos = "H", out.extra = "", message=FALSE, warning=FALSE)

library(shellpipes)
library(glmmTMB)
library(dplyr)
library(ggplot2)
library(vareffects); varefftheme()
library(ggpubr)

commandEnvironments()
makeGraphics()
 
set.seed(2121)
## Functions
plotEsize <- function(df, pos = 0.5, col_lab = ""){
	pos <- position_dodge(pos)
	p1 <- (ggplot(df, aes(x = reorder(term, -estimate), y = estimate, colour="Estimate"))
		+ geom_pointrange(aes(ymin = conf.low, ymax = conf.high), size=0.2, position = pos)
		+ geom_hline(yintercept=0,lty=2, size=0.2)
		+ coord_flip()
		+ labs(x="", y = "Estimate")
	)
}
```

\section{Introduction}

Generating predictions from regression models can be challenging and depends on whether the relationship between the response variable and the predictors is linear or nonlinear. For example, if response, $Y$, changes nonlinearly with the predictor variable, $X$, averaged response variable with respect to the predictor, $E(Y(X))$, does not necessarily equal the response at the mean of the predictor, $Y(E(X)$. This can be understood in relation to Jensen's inequality which states that, for a nonlinear function, $Y(X)$, then $E(Y(X)) > Y(E(X))$, if $Y(X)$ is positive second derivative; and $E(Y(X)) < Y(E(X))$ if $Y(X)$ is negative second derivative. Currently, existing R packages for predicting responses make distribution assumptions about the non-focal predictors, (for example, averaging the non-focal predictors at various levels of focal predictors), leading to potential biasness if the particular predictor is not well represented, or as a result of nonlinear averaging. To understand what we are trying to achieve, we first define the following:

\subsection{Prediction plots}

Notation :

\newcommand{\nset}[1]{#1_{\{n\}}}
\newcommand{\yref}{y_{\textrm{ref}}}
\newcommand{\cdist}{{D(\nset{x}|x_f)}}
\newcommand{\cdistprime}{{D(\nset{x}|x_{f'})}}
\newcommand{\xfprime}{x_{f'}}

- $x_f$: a value of the focal predictor
- $\nset{x}$: a vector of values of the non-focal predictors for a particular observation
- $\eta(x_f, \nset{x}) = \beta_f x_f + \sum \nset{\beta} \nset{x}$ = linear predictor (e.g. prediction on the log-odds scale)
- $g^{-1}()$: inverse-link function (e.g. logistic)
- $\cdist$: distribution of the non-focal predictors conditional on a particular value of the focal predictor
- $\beta_{fi}$: the coefficient describing the interaction(s) of the focal and non-focal parameters

The purpose and goal of a *prediction plot* seems fairly straightforward; for specified values of (a) focal predictor(s), we want to give a point estimate and confidence intervals for the prediction of the model for a "typical" (= random sample over the multivariate distribution of non-focal parameters) individual with those values of the predictors.

**BB**'s

("what is the expected probability of having clean water for a 25-year-old male"?). These are most of the problems we have actually been addressing with our bias correction stuff above. To do these computations, we need to take the values of the non-focal predictors (or their means and covariances) from the *conditional distribution*. (If the focal predictors are discrete, we condition on exact values; if they are continuous/have mostly unique values, we condition on appropriately sized bins.) In other words,
$$
\underset{\cdist}{\textrm{mean}}  g^{-1} \left(\eta(\nset{x},x_f) \right)
$$

Suppose we consider the values of non-focal predictors, to implement this:

- compute linear predictor of the non-focal predictors, $\nset{\eta} = \sum \nset{\beta} \nset{x}$
- find a list of vectors of observations of $\nset{\eta}$ associated with each value (bin) of the focal predictor, $\nset{{\eta_j}}$, $j = 1, 2, \cdots$
- for each $\nset{{\eta_j}}$:
	- compute $\hat{y}_j  = \textrm{mean} ~ g^{-1} \left(\beta_f x_{j_f} + \nset{{\eta_j}}\right)$

_What's the expected probability of having clean water for all $x$-year-old with the **corresponding** wealth index??_

**JD**'s and **SC**'s

- compute linear predictor of the non-focal predictors, $\nset{\eta} = \sum \nset{\beta} \nset{x}$
- for every value of the focal predictor, $x_{j_f}$:
	- compute $\hat{y}_j  = \textrm{mean} ~ g^{-1} \left(\beta_f x_{j_f} + \nset{\eta}\right)$

The major difference between **BB**'s and **JD**'s approach is that in the former case, we consider non-focal linear predictor corresponding to specific value (bin) of the focal predictor, while in the later case, for each value of the focal predictor, we consider linear predictor associated with entire population.

**Questions**

- What does the second case (JD's) represent?
- Which is the correct approach?

\subsection{Simulation}

We implement and apply these methods in the context of both simple generalized linear and mixed effect models, using simulated data sets -- for univariate and multivariate models.

We start with a univariate case where we have only one predictor.

\section{Simple fixed effect model}

\begin{align*}
\mathrm{logit(status} = 1) &= \eta \\ 
\eta &= \beta_0 + \beta_{\mathrm{A}} \mathrm{Age} + \beta_{\mathrm{W}} \mathrm{Wealthindex} \\
\mathrm{Age} &\sim \mathrm{Normal}(0.2, 1) \\
\mathrm{Wealthindex} &\sim \mathrm{Normal}(0, 1) \\
\beta_0 &= 1.5 \\
\beta_{\mathrm{A}} &= 1.0 \\
\beta_{\mathrm{W}} &= 2
\end{align*}

Considering $N=1e4$

```{r sim_simple}

simplesim <- function(N=1e4, beta0=1.5, betaA=1.0, betaW=2
		, age_sd=1, age_mean=0.2
		, wealth_sd=1, wealth_mean=0
	) {
	age <- rnorm(N, age_mean, age_sd)
	wealthindex <- rnorm(N, wealth_mean, wealth_sd)
	eta <- beta0 + betaA * age + betaW * wealthindex
	sim_df <- (data.frame(age=age, wealthindex=wealthindex, eta=eta)
		%>% mutate(status = rbinom(N, 1, plogis(eta)))
		%>% select(-eta)
	)
	return(sim_df)
}
sim_df <- simplesim()
true_prop <- mean(sim_df$status)
print(true_prop)
head(sim_df)
```

\subsection{Simple logistic model}

```{r simple_logistic}
simple_mod <- glm(status ~ age + wealthindex, data = sim_df, family="binomial")
```


\subsection{Variable predictions}


```{r pop_ave_fun, echo=FALSE}
binfun <- function(mod, focal, non.focal, bins=50, ...) {
	mf <- model.frame(mod)
	mm <- (mf
		%>% select_at(c(focal, non.focal))
	)
	check_df <- (mf
		%>% arrange_at(focal)
		%>% mutate(bin=ceiling(row_number()*bins/nrow(.)))
		%>% group_by(bin)
		%>% summarise_all(mean)
		%>% mutate(model="binned")
	)
	return(check_df)
}

```


```{r simple_pred_age, echo=FALSE, fig.cap="Prediction plots with N=1e4: JD's approach on the left; BB's on the right."} 
binned_df <- binfun(simple_mod, "age", "wealthindex")

## JD's approach
pred_age_jd <- predictionfun(simple_mod, "age", isolate=TRUE, include.nonfocal="jd", modelname="JD")
age_plot_jd <- (ggplot(pred_age_jd, aes(x=age, y=fit))
	+ geom_line()
	+ geom_line(aes(y=lwr), lty=2)
	+ geom_line(aes(y=upr), lty=2)
	+ geom_hline(yintercept=true_prop, lty=2, colour="grey")
	+ geom_point(data=binned_df, aes(x=age, y=status, color="binned"))
	+ labs(y="Probability")
	+ theme(legend.position="none")
)

## BB's approach
pred_age_bb <- predictionfun(simple_mod, "age", isolate=TRUE, include.nonfocal="bb", modelname="BB")
age_plot_bb <- (ggplot(pred_age_bb, aes(x=age, y=fit))
	+ geom_line()
	+ geom_line(aes(y=lwr), lty=2)
	+ geom_line(aes(y=upr), lty=2)
	+ geom_hline(yintercept=true_prop, lty=2, colour="grey")
	+ geom_point(data=binned_df, aes(x=age, y=status, color="binned"))
	+ geom_smooth(colour="black", size=0.5)
	+ labs(y="Probability")
	+ theme(legend.position="none")
)

ggarrange(age_plot_jd, age_plot_bb + rremove("ylab"), common.legend=FALSE)

```

Considering $N=1e5$ and then refit the model

```{r sim_simple_ne5}
sim_df <- simplesim(N=1e5)
true_prop <- mean(sim_df$status)
simple_mod <- glm(status ~ age + wealthindex, data = sim_df, family="binomial")
```

```{r simple_pred_age_ne5, echo=FALSE, fig.cap="Prediction plots with N=1e5: JD's approach on the left; BB's on the right."} 
binned_df <- binfun(simple_mod, "age", "wealthindex")

## JD's approach
pred_age_jd <- predictionfun(simple_mod, "age", isolate=TRUE, include.nonfocal="jd", modelname="JD")
age_plot_jd <- (ggplot(pred_age_jd, aes(x=age, y=fit))
	+ geom_line()
	+ geom_line(aes(y=lwr), lty=2)
	+ geom_line(aes(y=upr), lty=2)
	+ geom_hline(yintercept=true_prop, lty=2, colour="grey")
	+ geom_point(data=binned_df, aes(x=age, y=status, color="binned"))
	+ labs(y="Probability")
	+ theme(legend.position="none")
)

## BB's approach
pred_age_bb <- predictionfun(simple_mod, "age", isolate=TRUE, include.nonfocal="bb", modelname="BB")
age_plot_bb <- (ggplot(pred_age_bb, aes(x=age, y=fit))
	+ geom_line()
	+ geom_line(aes(y=lwr), lty=2)
	+ geom_line(aes(y=upr), lty=2)
	+ geom_hline(yintercept=true_prop, lty=2, colour="grey")
	+ geom_point(data=binned_df, aes(x=age, y=status, color="binned"))
	+ geom_smooth(colour="black", size=0.5)
	+ labs(y="Probability")
	+ theme(legend.position="none")
)

ggarrange(age_plot_jd, age_plot_bb + rremove("ylab"), common.legend=FALSE)

```
