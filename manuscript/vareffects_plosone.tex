% Template for PLoS
% Version 3.5 March 2018

% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 

% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx,grffile}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\fix}{{\bf \textcolor{red}{FIXME}}}
\newcommand{\JD}[1]{{\color{blue} \emph{#1}}}
\newcommand{\bmb}[1]{{\color{RubineRed!70!} \emph{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\def\code#1{\texttt{#1}}
\let\proglang=\textsf

\newcommand{\bX}{{\mathbf X}}
\newcommand{\bZ}{{\mathbf Z}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\boldeta}{{\boldsymbol \eta}}
\newcommand{\boldmu}{{\boldsymbol \mu}}

\newcommand{\nset}[1]{#1_{\{n\}}}
\newcommand{\yref}{y_{\textrm{ref}}}
\newcommand{\cdist}{{D(\nset{x}|x_f)}}
\newcommand{\cdistprime}{{D(\nset{x}|x_{f'})}}
\newcommand{\xfprime}{x_{f'}}

\newcommand{\yE}{{\mathrm E}}
\let\over=\overline

\newcommand{\bxo}{\bX^{\star}}
\newcommand{\bxof}{\bX_f^{\star}}
\newcommand{\ba}{{\mathbf A}}
\newcommand{\bao}{{\mathbf A}_0}
\newcommand{\baf}{{\mathbf A_f}}
\newcommand{\bafc}{{\mathbf A_{\bar{f}}}}


%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Describing the curves: uncertainty estimation and bias correction for predictor effects in simple and generalized linear (mixed) models} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Steve Cygu\textsuperscript{1},
Benjamin M. Bolker\textsuperscript{1,2},
Jonathan Dushoff\textsuperscript{1,2}
\\
\bigskip
\textbf{1} School of Computational Science and Engineering, McMaster University, Hamilton, Ontario, Canada
\\
\textbf{2} Department of Biology, McMaster University, Hamilton, Ontario, Canada
\\
\bigskip


% Use the asterisk to denote corresponding authorship and provide email address in note below.
* cygu@aims.ac.za

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}

In many applications which use generalized linear (mixed) models, outcome predictions or predicted probabilities are often of interest. For models that involve complex multiplicative interactions, additional non-focal predictors or nonlinear link functions, the estimated coefficient are not readily interpretable. A general way to summarize these kind of models is through predictions, which are not only dependent on choice of representative values of focal predictor but also sensitive to which values of non-focal predictors are chosen. The most common approach is generating predictions at a ``reference point'', usually the mean, of non-focal predictor, i.e., mean-based (centered) which could be considered as the effect of an ``average case'' in the population. In the presence of sources of bias such as additional non-focal predictors, nonlinear link functions, random effect terms, etc., mean-based approach generate predictor effects that are biased and may not be consistent with the observed quantities. An alternative is the whole-sample-based approach which estimates the average effect in the population. Moreover, anchored effects provide an alternative and a more clear way to describe uncertainty associated with the focal predictor of interest. In addition to theoretical and methodical comparison, using simulation, we illustrate the two approaches and show that they can produce substantially different results and that whole-sample-based approach can not only produce estimates consistent with the observed values, but also appropriate for bias correction. We also present an alternative way, anchored confidence intervals, to describe uncertainties associated with these predictions.

\section*{Definitions}

\begin{itemize}
\item \textbf{Input variables}: Scientific variables underlying an inference or exploration. The focal predictor we use for effects or predictions is an input variable.
\item \textbf{Model variables}: Variables that represent columns in the model matrix. Each input variable will correspond to one or more model variables. In particular, variables with more than two categories, or variables modeled with a spline or polynomial response, will correspond to more than one model variables.
\item\textbf{Model center:} A point corresponding to a column-wise mean of the model matrix (the mean of one or more model variables). The center point for a set of model variables corresponding to an input variable may not represent a possible value of the input variable.
\item \textbf{Reference point}: Value or values chosen for \emph{non-focal predictors}, when estimating effects. Typically the center point, but can instead be a population of quantiles or observations. 
\item \textbf{Anchor}: The value chosen for the \emph{focal predictor} when estimating effect confidence intervals (anchor choice does not affect the estimate). Typically chosen as the center point of the model variables corresponding to the focal input variable.
\end{itemize}

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}

\fix

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}

In many applications which use simple or generalized linear (mixed) models, outcome predictions or probabilities, are often of interest. Predictions provide a great way to summarize what the regression model is telling us and very useful for interpreting and visualizing model estimates. For example, in logistic regression models, the coefficient estimates are usually not easy to interpret and less informative. Since logistic models are nonlinear (due to the nonlinear link function), in multivariate models with interactions, the magnitude of the effect of change in the outcome depends on the values of the predictor of interest and other predictors. Hence, the conclusions one can make about the estimated effects greatly depends on how well we choose the values of the other predictors, i.e., \emph{non-focal} predictors.  

Both simple and generalized linear (mixed) models (GL(M)Ms) can examine very complex relationships, including nonlinear relationships between response and predictors, interactions between predictors (and via splines for example), and nonlinear transformations via link functions due to their flexibility. This flexibility comes at a cost, for example, complex multivariate models may risk misinterpretation, and miscalculation of quantities of interest. Also, coefficient estimates of models involving nonlinear link functions or interactions lose their direct interpretation \cite{leeper2017interpreting}, meaning that interpretation of derived quantities from these estimates requires some understanding of the specified model. An alternative is to explore the \emph{effects} of predictors on the predictions or probabilities of the outcome at various level of the predictor of interest -- \emph{prediction and effects} \cite{fox2009effect, leeper2017package, lenth2018package}. For example, as a way to plan, public health officials may want to know the effects of household income, wealth index, etc, on the predicted probability of having improved water services among slum dwellers.


When visually presented, prediction and effects provide a unified and intuitive way of describing relationships from a fitted model, especially complex models involving interaction terms or some kind of transformations on the predictors whose estimates are usually, but not always, a subject to less clarity of interpretation. Further, generating predictor and effect plots together with the associated confidence intervals for regression models has a number of challenges. In particular:
\begin{enumerate}
\item choice of representative values of \emph{focal} predictor and the \emph{reference point} for non-focal predictors especially in multivariate models
\item  propagation of uncertainty -- appropriate choice of \emph{anchor} for computing confidence intervals; and how to incorporate the uncertainty due to non-focal predictors 
\item bias in the expected mean prediction induced by the nonlinear transformation of the response variable (especially in GL(M)Ms)
\end{enumerate}

The most common way of dealing with the first challenge is taking unique levels of the focal (predictor of interest) predictor if discrete or taking appropriately sized quantiles (or bins) if continuous, and then calculating the predictions while holding non-focal (other predictors other than the focal predictor) predictors at their reference point (e.g., means) \cite{hanmer2013behind}. This generates -- \emph{predictions}, \emph{predictor effects}\cite{fox2009effect}, \emph{marginal predictions} \cite{leeper2017package} or \emph{estimated marginal means} \cite{lenth2018package}. In this article, we refer to this quantity as predictions since it should, for example, tell us what we would expect the response to be at a particular value or level of the predictor, for an ``average case''. More specifically, we compute the expected outcome by meaningfully holding non-focal predictors constant (or averaged in some meaningful way) while varying the focal predictor, with the goal that the outcome expected prediction represents how the model responds to the changes in the focal predictor. On the other hand, effects differs with predictions in the sense that the former is about how we describe the uncertainty around the latter and depends on the anchor. 

The commonly used \proglang{R} software packages (\pkg{emmeans} and \pkg{effects}) for generating predictions, by default, use the average of non-focal predictors as the reference point. However, there are a number of choices one can make when considering this approach -- for example, in the presence of interaction, averaging the interactions (averaging product of interacting model variables) versus product of the averages of the interacting model variables (default for \pkg{emmeans} and \pkg{effects}). We claim that neither of these two approaches is the most appropriate but we show that averaging the interaction closely matches the observed values. To illustrate this, consider models~\ref{eq:simple_inter_higher_no_interaction} and \ref{eq:simple_inter_higher} below, with $x_1$ as the focal predictor:

%
\begin{align}
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \epsilon \label{eq:simple_inter_higher_no_interaction}\\
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_{23}x_2x_3 + \epsilon \label{eq:simple_inter_higher}
\end{align}
%

We first simulate data with the two models, such that $x_{1,2,3} \sim \mathrm{Normal}(0, 1)$, $\beta_0 = 5$, $\beta_1 = -3$, $\beta_2 = 1$, $\beta_3 = 2$ and $\beta_{23} = 5$, and then compare the predictions from the \pkg{emmeans}, \pkg{effects} and our proposed alternative (\pkg{varpred}) to the ``true'' predictions and observed average i.e., $\bar{y}$, as shown in Fig~\ref{fig:justify_plots}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{justify_preds.inter.pdf}
\caption{{\bf A comparison of \pkg{emmeans}, \pkg{effects} and \pkg{varpred} predictions for $x_1$ on $y$ for models with and without interactions.}
The horizontal blue, green and black lines are the mean predictions, i.e., $\bar{\hat{y}}$, the red ones are the means of the simulated $y$, i.e., $\bar{y}$, while the vertical dotted grey lines are the means of the corresponding focal predictors. The grey points are the binned simulated $y$. The trend lines represents the corresponding $\hat{y}$ at various levels of $x_1$, while holding the other predictors at their average for the three mthods and the ``true'' predictions based on the simulation parameters. A: In the absence of interaction, the predicted mean, $\bar{\hat{y}}$, closely matches the simulated in all the three approaches, i.e., $\bar{y} \approx \bar{\hat{y}}$. Similarly, the predictions based on the three approaches closely matches the ``true'' predictions. B: Even with the simple interaction between non-focal predictors, we start seeing deviation in both predicted mean, $\bar{\hat{y}}$, and overall predictions from the simulated, $\bar{y}$, and ``true'' predictions, respectively, in two commonly used packages (\pkg{emmeans} and \pkg{effects}), but not, the proposed \pkg{varpred}.}
\label{fig:justify_plots}
\end{figure}

In the absence of interaction (model~\ref{eq:simple_inter_higher_no_interaction}), the three approaches produce similar estimates, which match the simulated values, Fig~\ref{fig:justify_plots}A. However, in the presence of interaction, even as simple as the one in model~\ref{eq:simple_inter_higher}, the estimates starts to differ. In particular, \pkg{emmeans} and \pkg{effects} give similar estimates ($\bar{\hat{y}}$) but different from the \pkg{varpred}'s which, however, is very close to the simulated average ($\bar{y}$), Fig~\ref{fig:justify_plots}B. To generate Fig~\ref{fig:justify_plots}A, all the three packages averages non-focal predictors $x_2$ and $x_3$. On other hand, to generate Fig~\ref{fig:justify_plots}B, the difference in the estimates lies on how each of the packages average the interaction term ($x_2x_3$). In particular \pkg{emmeans} and \pkg{effects} uses \emph{input variables} to compute $\bar{x_2}\bar{x_3}$ while \pkg{varpred} uses \emph{model variables} to compute $\over{x_2x_3}$.

Sometimes, one may be interested in the uncertainties associated with the focal predictor of interest only, excluding other uncertainties due to other non-focal predictors -- effects. However, currently, the two packages do not provide straightforward way do achieve this. To illustrate this, we generate predictions of $x_2$ from model~\ref{eq:simple_inter_higher_no_interaction} together with the associated $95\%$ confidence bands, as shown in Fig~\ref{fig:justify_ci_plots}. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{justify_preds.isolate.pdf}
\caption{{\bf A comparison of conventional $95\%$ confidence bands and effects.} The description of horizontal, vertical and trend lines remain the same as above. The wider dotted blue curves overlaying the green curves corresponds to the conventional confidence bands around the predcitions from \pkg{emmeans} and \pkg{effects}, while the narrower black curves crossing at the mean of the focal predictor, i.e., the model center, corresponds to the effects of the focal predictor from \pkg{varpred}. For simple models, the isolated confidence bands crosses at the model center. The wider curves incorporates not only the uncertainties due to intercept term but also other non-focal predictors. On the other hand, effects (narrower crossing lines) only take into account the main effect uncertainty of the focal predictor.}
\label{fig:justify_ci_plots}
\end{figure}

For \pkg{emmeans} and \pkg{effects}, the confidence bands are much wider because they include uncertainties associated with the intercept and non-focal predictors, but narrower and crosses at the mean of the focal predictor, i.e., \emph{center point or mean-based}, in \pkg{varpred}. In other words, with \pkg{varpred}, we are able to generate effects indicating zero uncertainty at the value of the focal predictor we are more certain about, i.e., mean of the model variable corresponding to the focal predictor (center point or mean-based). For simple models, the point where the confidence bands crosses is the model center and it corresponds to anchor we choose, in this case, the mean of the focal predictor.


When dealing with nonlinear link functions or models with multiple non-focal predictors, the correct predictions, for example, are even much harder to estimate. One approach is to make predictions on the transformed scale (linear predictor scale), and then back-transform to the original scale. However, the back-transformation may either result in biased predictions or requires some approximation. In particular, bias in expected mean prediction induced by nonlinear transformation of the response variable can lead inaccurate predictions. An alternative to mean-based reference point, i.e., averaging non-focal predictors, is the \emph{whole-sample-based} approach, discussed in detail later, which involves computing the prediction over the population of non-focal predictors and then averaging across the values of the focal predictor \cite{hanmer2013behind}. 


The main purpose of this article is to discuss and implement various approaches for computing predictions and provide an alternative method for computing the associated confidence intervals, i.e., effects. We further explore and demonstrate, using simulated data, approaches for correcting bias in predictions for GL(M)Ms involving nonlinear link functions.


\section*{Quantities of interest}

Several quantities of interest may be derived from regression models. The first one is the coefficient estimates. Others are \emph{input and model variables}, \emph{model matrix}, \emph{model center}, \emph{reference point} and \emph{anchor}.

\subsection*{Input variables}

These are scientific variables underlying an inference or exploration. The focal and non-focal predictors we use for effects or predictions are an input variables.

\subsection*{Model matrix}

Model matrix refers to the design matrix whose rows include all combination of variables appearing in the interaction terms, along with the typical values of the focal and non-focal predictors. 

\subsection*{Model variables}

These are variables that represent columns in the model matrix. Each input variable may correspond to one or more model variables. In particular, variables with more than two categories, or variables modeled with a spline or polynomial response, will correspond to more than one model variables.

\subsection*{Model center} 

A point corresponding to a column-wise mean of the model matrix (the mean of one or more model variables). We also refer to this as mean-based approach. The center point for a set of model variables corresponding to an input variable may not represent a possible value of the input variable -- the case in Fig~\ref{fig:justify_plots}. In simple linear models (without) interaction, the input variable can be used as a proxy.

\subsection*{Reference point}

Corresponds to the value or values chosen for non-focal predictors, when estimating the predictions. Typically the center point (mean-based), but can instead be a whole-sample of quantiles or observations. Appropriately choosing the reference point is very crucial in generating ``correct'' predictions especially when dealing with non-linear link functions.


\subsection*{Anchor}

The value chosen for the focal predictor when estimating effect confidence intervals (anchor choice does not affect the main prediction estimates). Typically chosen as the center point of the model variables corresponding to the focal input variable but other sensible values can be chosen too.

If we are trying to focus on effects, the anchor from which we calculate CIs becomes important. This is not true for the more conventional prediction plots: we can choose any anchor, e.g., zero-anchored, but model center seems like a natural, stable choice, i.e., center-anchored. In Fig~\ref{fig:justify_anchors}, we compare effects plots anchored at the minimum and center (mean) of the focal predictor.

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{justify_anchors.combined.pdf}
\caption{{\bf A comparison of min-based and center- (mean-) based anchors.} The center-based effects are based on the mean of the focal predictor while the min-anchored effects are based on the minimum of the focal predictor. As we mentioned earlier, the choice of the anchor do not affect the main estimates (central trend lines).} 
\label{fig:justify_anchors}
\end{figure}

If we have more than one model variables for our focal predictor, however, the model center does not correspond to any given value of the variable, and we don't get the nice crossing we get in simpler cases. See Fig~\ref{fig:pred_cubic_plots}.


\subsection*{Prediction- vs. effect- styled plots}

A prediction-style plot is about predicting observations (or the ensemble mean of observations) for a given value of the focal predictor. For this, we want to use the classic curved confidence intervals (or prediction intervals). A prediction-style plot will generally focus on \emph{total} effects, and so will often be suited for a univariate fit. An effect-style plot, however, is an attempt to visualize the effect of a focal predictor. In this case, we suggest the narrower confidence intervals. Here, it makes equal sense to think about the total effect of our focal predictor, from a univariate model, or the direct effects – after controlling for whatever – from a multivariate model. See Fig~\ref{fig:justify_ci_plots}.

\subsection*{Predictions vs. effects}

We would like to summarize by talking about predictions vs. effects. Predictions are the standard curves; they tell us what we expect to see if we know the value of the focal predictor.

The lines inside (effects) are a little different: they are a good way to visualize the effect of the focal predictor. Unlike predictions, where the uncertainty captures the uncertainty in the whole model, the effects plot focuses on uncertainty in the effect of the focal predictor only -- thus, for example, if an effect is barely significant (P exactly 0.05), one end of the CI will be exactly a horizontal line for the effects plot, but not for the prediction plot. Also, the uncertainty curves for the effect, but not for the prediction, depend on the ``anchor''.



\section*{Statistical background}

To get an intuition of how conditioning on the mean values of non-focal predictors work, suppose we are interested in predictions at the levels of a particular predictor, i.e., focal, $x_f$, from the set of predictors. As a starting point, assume that the model has no interaction terms. The idea is to choose a reference point for the values of non-focal predictors. For example, fixing the values of non-focal predictor(s) at some typical values -- typically determined by averaging (for now) in some meaningful way, for example, arithmetic mean  for continuous and average over the levels of the factors for categorical non-focal predictors. We refer to this as centered or mean-based approach. The most convenient way to achieve this is by constructing $\bX^\star$ by averaging the columns of non-focal predictors in model matrix $\bX$, and together with appropriately chosen values of focal predictor.


Consider a simple linear model with linear predictor $\eta = \bX\bbeta$ and let $g(\boldmu) = \boldeta$ be an identity link function (in the case of simple linear model), where $\boldmu$ is the expected value of response variable $y$. Let $\hat{\bbeta}$ be the estimate of $\bbeta$, together with the estimated covariance matrix $\boldsymbol{\Sigma} = V(\hat{\bbeta})$ of $\hat{\bbeta}$. Let $\mathbf{X^*}$ be the model matrix, inheriting most of its key properties, for example transformations on predictors and interactions from the model matrix, $\mathbf{X}$. Then the quantity $\hat{\boldeta}^\star = \bX^\star\hat{\bbeta}$ is the prediction with respect to the focal predictor in question \cite{fox2009effect}.

An alternative formulation of $\hat{\boldeta}^\star$ involves expressing the linear predictor as the sum of the focal and non-focal predictors' linear predictor. In particular, 

\begin{align}\label{eq:eta_mean}
\eta^\star(x_f, \nset{{\bar{x}^\star}}) &= \beta_f x_f + \sum \nset{\beta} \nset{{\bar{x}^\star}} \\
\hat{y}_f  &= g^{-1} \left(\eta^\star(x_f, \nset{{\bar{x}^\star}})\right)
\end{align}
where $\nset{{\bar{x}^\star}}$ are the appropriately averaged entries of non-focal predictors and $x_f$ is a vector of values of the focal predictors for a particular observation.


\subsection*{Dealing with higher order interactions}

Higher order terms such as interactions, splines, polynomials, etc., can be within the focal predictor, within (between) non-focal predictor(s), between the focal and non-focal predictor(s). To distinguish the three, suppose the model which describes the hypothetical simulated household size simulation based on a number of socio-demographic factors is

\begin{align}\label{sim:lm_cubic}
\mathrm{hh~size}_i &= \beta_0 + \beta_{\mathrm{A_1}}\mathrm{Age}_i + \beta_{\mathrm{A_2}}\mathrm{Age}^2_i + \beta_{\mathrm{A_3}}\mathrm{Age}^3_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\end{align}

In the first case, with $Age$ as the focal predictor, the interaction is within its polynomial terms (and 3 model variables associated with it). In this case, each of the polynomial terms (model variables) of the focal predictor are evaluated independently across the chosen levels of the focal predictor, $Age_i$. Specifically, the higher order terms are treated as additional columns of the model matrix evaluated with the same values of the focal predictor, while non-focal predictors are fixed at their reference point as discussed in the previous section. In this case, the predictions associated with $Age$ on the linear predictor scale becomes

\begin{align*}
\eta^\star(\mathrm{Age}_i, \nset{{\over{\mathrm{Wealthindex}}}}) &= \beta_0 + \beta_{\mathrm{A_1}}\mathrm{Age}_i + \beta_{\mathrm{A_2}}\mathrm{Age}^2_i + \beta_{\mathrm{A_3}}\mathrm{Age}^3_i\\
	& + \beta_{\mathrm{W}}\over{\mathrm{Wealthindex}}
\end{align*}

In the second case, with $Wealthindex$ as the focal predictor, the higher order interactions in non-focal predictor, $Age$, are treated simply as additional columns in the variable space (of model matrix) and appropriate choice of reference point applies just like in the models without interactions. For instance, in mean-based approach, we simply average all non-focal interaction terms. Thus

\begin{align*}
\eta^\star(\mathrm{Wealthindex}_i, \nset{{\{\over{\mathrm{Age}}, \over{\mathrm{Age}^2}, \over{\mathrm{Age}^2}\}}}) &= \beta_0 + \beta_{\mathrm{A_1}}\over{\mathrm{Age}} + \beta_{\mathrm{A_2}}\over{\mathrm{Age}^2} + \beta_{\mathrm{A_3}}\over{\mathrm{Age}^3}\\
	& + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i
\end{align*}

Lastly, consider model~\ref{eq:simple_inter_higher}, with the interactions between the focal predictor, $x_2$, and non-focal predictor, $x_3$. In this case, the non-interacting predictors are treated as non-focal predictors, with appropriate reference points. However, for the interacting predictors, we first choose representative values for the focal predictor and some particular values of the interacting predictor similar to the way we choose values for the focal predictor or predetermined. In our example, suppose we pick $i$ and $j$ unique values of the focal predictor, $x_2$ and interacting predictor $x_3$, respectively, then the prediction on linear predictor scale is given by

\begin{align*}
\eta^\star(x_{2i}, x_{3j}, \nset{{\bar{x}^\star}}) = \beta_0 + \beta_1 \bar{x}_1 + \beta_2x_{2i} + \beta_3x_{3j} + \beta_{23}x_{2i}x_{3j}.
\end{align*}

In general, our formulation, even for more complicated interactions, follow these three basic principles -- interaction within focal predictor, interaction between non-focal predictors and interaction between focal and non-focal predictors.


\section*{Uncertainty estimation}

What about the confidence intervals (CI)? The limits of the confidence intervals are points, not mean values. In principle, every value of focal predictor has a different CI. The conventional way to compute variances for predictions is $\sigma^2 = \textrm{Diag}(\bX^\star \boldsymbol{\Sigma} \bX^{\star\top})$ \cite{lenth2018package, fox2009effect}, so that the confidence intervals are $\eta \pm q\sigma$, where $q$ is an appropriate quantile of Normal or t distribution. This generates predictions-styled CIs and incorporates all the uncertainties -- including the uncertainty due to the intercept and non-focal predictors.  But what if we are only interested in the uncertainty as a result of the focal predictor, so that the confidence intervals are $\eta \pm q \sigma_f$? We call this anchored CIs and generates effects-styled CIs.

Currently, commonly used \proglang{R} packages for constructing predictions, by default, do not exclude the uncertainties resulting from non-focal predictors when computing the CIs. A non-trivial way to exclude uncertainties associated with non-focal predictors in some of these packages is to provide a user defined variance-covariance matrix with the covariances of non-focal terms set to $0$ -- \emph{zeroing-out} variance-covariance matrix. This only works when the input predictors are \emph{centered} prior to model fitting, in case of numerical predictors, and even much complicated when the predictors are categorical. We first describe the variance-covariance based approach and then discuss our proposed method which is based on the model center and does not require input predictors to be scaled prior to model fitting.


\subsection*{Variance-covariance}

The computation of $\hat{\eta}^\star$ remains the same as described above. However, to compute $\sigma$, $\boldsymbol{\Sigma}$ is modified by \emph{zeroing-out} (the variance-covariance of all non-focal predictors are set to zero) variances of non-focal terms. Although this is the simplest approach, it requires centering of continuous, i.e., $x_c = x - \bar{x}$, predictors prior to model fitting and proper way to average categorical predictors.

\subsection*{Model center based}

Let $\bxo$ be a centered model matrix containing appropriately chosen values of focal predictors and centered non-focal predictors, and let $\ba$ be the anchor matrix, with same dimensions as $\bxo$ and values in each columns repeated $n$ times. Also, let $\baf$ be the column of $\ba$ corresponding to the focal predictor, the rest of the columns corresponds non-focal predictors with same values as in $\bxo$. Any appropriate values (within the range of focal values) can be chosen for $\baf$ but for model center (center-anchored), we have $\bafc$ . Thus $\sigma_f^2 = \textrm{Diag}((\bxo - \bafc) \boldsymbol{\Sigma} (\bxo - \bafc)^\top)$. We can see that $\forall ~\bxof=\baf$, $\sigma_f^2 = \boldsymbol{0}$. This is true for the center-anchored provided $\bxof=\bafc$. In other words non-focal terms (model variables) in $\bX^{\star}_{c} = (\bxo - \bafc)$ are all zero in simple models without interactions but are isolated (narrower) around the model center in models involving complex interactions. In addition, the computation of $\bX^{\star}_c$ impacts only on the intercepts and non-focal model variables, i.e., the slopes and variance of the focal predictors are not affected. This means that we can still generate effects without necessarily centering the predictors prior to model fitting.

%% \subsection*{Is center-anchored better?}
%% 
%% We want to compare the amount of uncertainty when there is/no anchor. Let $\bao = \boldsymbol{0}$ be the anchoring matrix with all entries in $0$s, and let $\bafc$ be the center-anchored matrix defined above. Then entries in $(\bxo - \bao) \geq (\bxo - \bafc)$.
%%

\section*{Bias correction}

In many applications, it usually important to report the estimates that reflect the expected values of the untransformed response. This is not a major issue in simple linear models. However, when dealing with nonlinear link functions, it is even harder to generate correct predictions that reflect this due to the bias in the expected mean induced by the nonlinear transformation of the response variable. In such cases, bias correction is needed when back-transforming the predictions to the original scales. Most common approach for bias-adjustment is second-order Taylor approximation \cite{lenth2018package, duursma2003bias}. Another potential source of bias comes from additional non-focal predictors, especially in models with nonlinear link functions. Here, we describe and implement a different approach, whole-sample-based approach for bias correction.


\subsection*{Whole-sample-based approach for bias correction}

The most precise (although not necessarily accurate!) way to predict is to condition on values of the focal predictor and make predictions for all observations (members of the population) \cite{hanmer2013behind}. A key point is that the nonlinear transformation involved in these computations is always \emph{one-dimensional}; all of the multivariate computations required are at the stage of collapsing the multidimensional set of predictors for some subset of the population to a one-dimensional distribution of $\eta^\star(x_f, \nset{x})$, which is a function of focal predictor and the observed values of non-focal predictors, as opposed to the definition in Equation~\ref{eq:eta_mean}. More specifically:

\begin{itemize}
\item compute linear predictor associated with non-focal predictors, $\nset{\eta} = \sum \nset{\beta} \nset{x}$
\item compute linear predictor associated with the focal predictors, $\eta_{jf} = \sum{\beta_f x_{jf}}$
\item for every value of the focal predictor, $\eta_{jf}$:

\begin{align}\label{eq:pop_eta} 
\eta_j^\star(\eta_{jf}, \nset{\eta})  &= \eta_{jf} + \nset{\eta} \nonumber \\
&= \eta_j^\star(x_f, \nset{x})
\end{align}
\end{itemize}

Once Equation~\ref{eq:pop_eta} is computed, one can back-transform the estimates to the original scale and the average over the levels of the focal predictors, $j$:

\begin{align}\label{eq:pop_response} 
\hat{y}_f  &= \textrm{mean} ~ g^{-1} \left(\eta_j^\star(x_f, \nset{x})\right)
\end{align}

We make similar adjustments to compute the variances of the predictions at every level of the focal predictor:

\begin{align}
\sigma_{jf}^2 = \textrm{Diag}(\bX^\star_{jc} \boldsymbol{\Sigma} \bX^{\star\top}_{jc})
\end{align}
where $\bX^{\star}_{jc} = \{\bX_{jf}^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$ and 

\begin{align}
\mathrm{CI}_f = \mathrm{mean} ~ g^{-1} \left(\eta_j^\star(x_f, \nset{x}) \pm q\sigma_{jf}\right)
\end{align}

For models with random effects components, we make further adjustment to correct for bias induced by the random effects terms. In the population approach, we treat the random effects terms as additional non-focal predictors and simply make adjustment to Equation~\ref{eq:pop_eta}. In particular

\begin{align}\label{eq:pop_eta_re} 
\tau &= \bZ b \nonumber \\
\eta_j^\star(x_f, \nset{x}, \tau)  &= \eta_j^\star(x_f, \nset{x}) + \tau
\end{align}
where $\bZ$ and $b$ are the design matrix and a vector of random effects, respectively.

\section*{Mean-based vs. whole-sample-based}

As mentioned above, the two common choices to generate predictions are: 1) setting non-focal predictors to their mean -- centered- (mean-) based; and 2) using the entire population of non-focal predictors -- whole-sample-based. If the link function is nonlinear and/or the model has complex higher order interactions, the average of the predictions evaluated at the mean of non-focal predictors and the average of predictions evaluated at the population level (and then averaged) of non-focal predictor are not equivalent, i.e.,

\begin{align}\label{eq:compare_anchored_pop_based}
g^{-1} \left(\eta_j^\star(\bar{x}_f, \nset{\bar{x}})\right) \neq \frac{1}{n} \sum_{i=1}^n{ g^{-1} \left(\eta_j^\star(\bar{x}_f, \nset{x})\right)}.
\end{align}

If $g()$ is strictly concave or strictly convex, one can use Jensen's inequality to determine which side of Equation~\ref{eq:compare_anchored_pop_based} is smaller or larger. However, for non-fully convex or concave link function, one can use Taylor series expansion to approximate the range over which the two are smaller or smaller than the other \cite{hanmer2013behind}. 


As stated above, in simple linear models without interactions, the effect is constant, so both approaches yield similar results. However, picking a single value, e.g., mean of the predictor, on which to draw conclusions about the effect can be problematic, unrealistic or not contained in or representative of the population. In addition, the mean-based approach fails to use every values of non-focal predictors hence not utilizing the full potential of the information contained in the data. This may limit the inferences we can make about the entire population. In general, the mean-based approach provides the predictions of an average case, whereas, the whole-sample-based approach, summarizes the predictions over the entire population -- and in some applications, the effect of an average case might not be generalizable to the entire population, especially, if the average does not represent the population. The whole-sample-based focuses on specific observations since the prediction is first obtained for each observation and then averaged across the levels of the focal predictor.

Another potential concern with the mean-based approach arises in situations where direct naive use leads to rare or meaningless basis for generalization. For example, setting dummy categorical variables in the model matrix to their means, which, by default, sets them to their sample means or observed proportions. For example, a dummy variable for christian household heads may be set to $0.2$, translating to prediction for an household head who is $20\%$ christian. This problem may extend to models with other complex interaction terms \cite{hanmer2013behind}.

Whole-sample-based approach is not entirely foolproof. For instance, similar to the mean-based approach, in the case of continuous focal predictors, choosing the representative values of the focal predictors can be very challenging especially if the cases are not evenly distributed around the minimum and the maximum values or within some subgroups defined in the population. In addition, whole-sample-based approach can be very computationally intensive for large datasets.

\section*{Simulation examples}

We now illustrate the construction of prediction-styled and effect-styled plots and also demonstrate that the mean-based and whole-sample-based approaches produce different results in models with nonlinear link functions and/or additional source(s) of potential bias. In addition, we demonstrate that whole-sample-based approach can be used for bias correction.

\subsection*{Predictions-styled and effects-styled plots}

As mentioned above, in some applications, one may be interested in uncertainties associated with the focal predictor only. In that case, it is important to exclude all the uncertainties as a result of non-focal predictors and only show confidence intervals describing the predictor of interest. To illustrate this, we consider the simulation model described in \nameref{S1_Appendix}. 

We start by generating two predictions from the fitted model \nameref{S1_Appendix}: 1) one with cubic polynomial of \code{age} as a focal predictor; and 2) one with a simple focal predictor (non-polynomial) of \code{Wealthindex}. Then we compared non-anchored (more of predictions) and mean-anchored (effects) confidence intervals around the predicted values (see Fig~\ref{fig:pred_cubic_plots}).

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{cubic_predictors_preds.ggp.pdf}
\caption{{\bf Prediction-styled and effect-styled plots for cubic polynomial and simple focal predictors.} A: The focal predictor is a cubic polynomial. B: The focal predictor is not a polynomial function, implying that the complex interaction is in non-focal predictor. In both cases, the mean-anchored effects are narrower than the non-anchored (predictions), but crosses at the mean of the focal predictor (model center) only in the case of simple (non-polynomial) focal predictor -- B. In the case of cubic polynomial focal predictor, the model center is not a point in the predictor space, i.e., mean of the focal predictor, but a combination of all the model variables in the cubic polynomial, consequently, mean-anchored curves do not cross at a point -- A. The horizontal black and red dotted lines are the observed and predicted average household size, i.e., $\over{hh~size}$ and $\over{\widehat{hh~size}}$, respectively; the inner red dotted curves and crossing lines are the anchored confidence bands for A and B, respectively, while the outer blue curves are the non-anchored confidence bands in both cases. The central curve and trend line, in A and B, respectively, are the predicted household size based on age and wealth index, respectively.}
\label{fig:pred_cubic_plots}
\end{figure}

In the absence of complex higher order interaction terms or transformations on the focal predictor in simple linear models, we would expect, at the model center, the observed average, $\over{hh~size}$, and the average of the predictions, $\over{\widehat{hh~size}}$, to be identical and cross with the mean-anchored confidence bands (effects), at the model center (see Fig~\ref{fig:pred_cubic_plots}B). On the other hand, if the focal predictor is characterized by complex interactions or any other form of transformation, the model center is not necessarily a point in the predictor space. In this case, the mean-anchored confidence bands will be narrower than the non-anchored ones but not necessarily crossing at the model center (see Fig~\ref{fig:pred_cubic_plots}A).

\subsection*{Bias correction}

Mean-based and whole-sample-based approaches can produce very different results in models with nonlinear link functions with additional sources of potential bias such as additional non-focal predictor(s), random effect terms or complex interactions. To demonstrate this, we considered a two predictor binary outcome simulation described in \nameref{S2_Appendix}, such that the age effect is way smaller than that of the wealth index, and compared their effects on the predicted probability of improved water quality as shown in Fig~\ref{fig:pred_bin_plots}. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{glm_two_predictor_preds.ggp.pdf}
\caption{{\bf Mean-based and whole-sample-based predictions.} The two approaches produce relatively different estimates in each of the predictors but the difference is more pronounced when the effect of the particular focal predictor has no strong effect -- \emph{on the left}. In both cases, \emph{right} and \emph{left}, the average predicted probability of improved water based on whole-sample-based approach is very close to the observed proportion in the simulated data. The horizontal blue, black and the red dotted lines are the average mean-based, whole-sample-based predicted probability and observed proportion of improved water quality, respectively. The vertical dotted black line represents the mean of the focal predictor, and the point at which it crosses the red dotted line represents the expected ``perfect'' prediction at the model center. The grey points are binned observations -- observed proportions of improved water quality in each bin. We would expect all the curves, horizontal, and vertical lines to cross at the model center but due the nonlinear averaging (at least in the bias corrected whole-sample-based approach) of the predictions on the response scale, this may not always be the case.} 
\label{fig:pred_bin_plots}
\end{figure}

If there was no effect of nonlinear averaging, then we would expect the observed proportion and the average of the predicted probabilities to cross at the model center as we see in Fig~\ref{fig:pred_cubic_plots}B. However, the differences we see in Fig~\ref{fig:pred_bin_plots} are due to nonlinear averaging since both observed status and predicted probabilities are averaged on the response scale as opposed to link scale. In particular, if the range of values are bigger than $0.5$ (seemingly the case here), then we would expected the averages to be slightly higher than what we would expect at the model center, and vice versa.

\subsection*{Mediated effect}

Using simple indirect mediation simulation model described in \nameref{S3_Appendix}, fitted two models -- \emph{non-mediated} which models \emph{z} as a function of \emph{x} (univariate model) and the \emph{mediated} which models \emph{z} as a function of both \emph{x} and \emph{y} (multivariate model). In both cases, we compared mean-based and the whole-sample-based predictions, as shown in Fig~\ref{fig:pred_mediated_plots}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{mediate_preds.ggp.pdf}
\caption{{\bf Mean-based and whole-sample-based predictions for mediated effects.} These figures compare the mean-based and whole-sample-based predictions for the mediated (multivariate) and non-mediated (univariate) models. A: In the absence of mediator variable, both mean-based and whole-sample-based approaches predictors are identical since there is no any other additional (non-focal predictors) sources of bias. Consequently, for both approaches, the average predicted probability is very close to the observed proportion. B: When the mediator variable is included, there is no direct effect of $x$ on $z$ and as a result the predictions do not necessarily align with the observations. However, the whole-sample-based approach still closely approximates the average proportion in the simulated data. The horizontal blue, black and the red dotted lines are the average mean-based, whole-sample-based predicted probability and observed proportion, respectively. The vertical dotted black line represents the mean of the focal predictor, and the point at which it crosses the red dotted line represents the expected ``perfect'' prediction at the model center. The grey points are the binned observations.}
\label{fig:pred_mediated_plots}
\end{figure}

From Fig~\ref{fig:pred_mediated_plots}A, we see what we would expect in the absence of additional sources of bias, even though in the simulation, the effect of $x$ on $z$ is mediated through $y$. By ignoring $y$ in the model, we are still able to capture the effect of $x$ and closely match the observed values using both approaches. However, if there were additional non-focal predictors, we would expect to see the differences similar to those in Fig~\ref{fig:pred_bin_plots}. Including both $y$ and $x$ ``dilutes'' the direct effect of $x$ on $y$ and as a result, our prediction do not necessarily match the observed binned observations (see Fig~\ref{fig:pred_mediated_plots}B).

\section*{Discussion}

Our simulations examples majorly focused on simple linear and logistic models due their wide range of usage and application. In addition, these models act as a starting point for building other complex models, including mixed effect models and models with categorical predictors. However, the logic for extension of approaches to more complex models, including other forms of nonlinear link functions is very straightforward. In addition, our \proglang{R} package implementation already extends to and supports most of the nonlinear link functions and mixed model framework, including multivariate binary outcome models.

Although commonly used \proglang{R} software packages, by default, implements mean-based approach, our simulation results demonstrated that the whole-sample-based approach has a potential of yielding results which are more consistent with the observed data. We would, therefore, argue that the use of mean-based approach should have some theoretical justification, especially in complex models. 

\section*{Conclusion}

Generating outcome predictions or predicted probabilities from simple and generalized linear (mixed) models is not only important but also, generating quantities which are consistent with the observed values should be of interest. However, many studies still report coefficient estimates from generalized models like probit, logistic, etc.,\cite{hanmer2013behind}, which are subject of less clarity due to lack of direct link to the outcome of interest.

The argument and results we present in this paper supports a greater need for a shift on focus on what and how to present predictions from generalized models. For example, we believe that effect-styled confidence intervals could provide more clarity concerning the uncertainty due to the predictor of interests as opposed to the conventional way of incorporating everything. 

From our theoretical, methodological and simulation results, researchers using these kind of models should, in the absence of theoretical justification, report predictions based on whole-sample-based approach or at least attempt to do a comparison of the two approaches before settling on the most appropriate in answering their research question. Moreover, we provide \proglang{R} package, \pkg{vareffects}, which implements these methods and is available on github (\href{https://github.com/mac-theobio/effects}{https://github.com/mac-theobio/effects}).


\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Appendix.}
\label{S1_Appendix}
{\bf Cubic polynomial interaction simulation.} Consider an hypothetical simulation which simulates household size as a function of household wealth index and cubic function of the age of the household head, specified as follows:

\begin{align}\label{sim:lm_cubic}
\mathrm{hh~size}_i &= \beta_0 + \beta_{\mathrm{A_1}}\mathrm{Age}_i + \beta_{\mathrm{A_2}}\mathrm{Age}^2_i + \beta_{\mathrm{A_3}}\mathrm{Age}^3_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i + \epsilon_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\epsilon_i &\sim \mathrm{Normal}(0, 10) \nonumber\\
\beta_0 &= 20 \nonumber\\
\beta_{\mathrm{A}_1} &= 0.1 \nonumber\\
\beta_{\mathrm{A}_2} &= 0.8 \nonumber\\
\beta_{\mathrm{A}_3} &= 0.3 \nonumber\\
\beta_{\mathrm{W}} &= -0.5 \nonumber\\
i &= 1,\cdots, 100
\end{align}


\paragraph*{S2 Appendix.}
\label{S2_Appendix}
{\bf Binary outcome simulation.} Consider a simple simulation for improved water quality in Nairobi slums, such that the status is $1$ for improved and $0$ for unimproved water quality. In additional to the focal predictor, age of the household head, we add wealth index. In particular:

\begin{align}\label{sim:glm_two_pred}
\mathrm{status}_i &\sim \mathrm{Bern}(\mathrm{P_i}) \nonumber\\
\mathrm{logit}(\mathrm{P_i}) &= \eta_i \nonumber\\
\mathrm{\eta}_i &= \beta_0 + \beta_{\mathrm{A}}\mathrm{Age}_i + \beta_{\mathrm{W}}\mathrm{Wealthindex}_i \nonumber\\
\mathrm{Age}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\mathrm{Wealthindex}_i &\sim \mathrm{Normal}(0, 1) \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{\mathrm{A}} &= 0.5 \nonumber\\
\beta_{\mathrm{W}} &= 1.5 \nonumber\\
i &= 1,\cdots, 10000
\end{align}

\paragraph*{S3 Appendix.}
\label{S3_Appendix}
{\bf Mediated effect simulation.} Next, we consider a simple indirect mediation simulation such that $x$ has direct effect on $y$ which in turn has effect on $z$ but $x$ has no direct effect on $y$, i.e., $x \rightarrow y \rightarrow z$. In particular:

\begin{align}\label{sim:simple_mediate}
z_i &\sim \mathrm{Bern}(\mathrm{P_i}) \nonumber\\
\mathrm{logit}(\mathrm{P_i}) &= \eta_i \nonumber\\
\eta_i &= \beta_0 + \beta_{xz} x_i + \beta_{yz} y_i \nonumber\\
y_i &= \rho x_i + \sqrt{1-\rho^2} y_y \nonumber\\
x_i &\sim \mathrm{Normal(0, 1)} \nonumber\\
y_y &\sim \mathrm{Normal(0, 1)} \nonumber\\
\rho &= 0.8 \nonumber\\
\beta_0 &= 5 \nonumber\\
\beta_{xz} &= 0.2 \nonumber\\
\beta_{yz} &= 1.5 \nonumber\\
i &= 1,\cdots, 10000
\end{align}


\section*{Acknowledgments}

This work was supported by a grant to Jonathan Dushoff from the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery.

\section*{Author Contributions}

\textbf{Conceptualization:} Jonathan Dushoff

\noindent\textbf{Software:} Steve Cygu, Benjamin M. Bolker

\noindent\textbf{Supervision:} Jonathan Dushoff, Benjamin M. Bolker

\noindent\textbf{Writing – original draft:} Steve Cygu


\nolinenumbers



% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 

\bibliography{vareffects_plosone}



\end{document}

