% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}
\newcommand{\fix}{{\bf FIXME}}
\newcommand{\JD}[1]{{\color{blue} \emph{#1}}}
\newcommand{\bmb}[1]{{\color{RubineRed!70!} \emph{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\def\code#1{\texttt{#1}}
\let\proglang=\textsf

\newcommand{\bX}{{\mathbf X}}
\newcommand{\bZ}{{\mathbf Z}}
\newcommand{\bbeta}{{\boldsymbol \beta}}
\newcommand{\boldeta}{{\boldsymbol \eta}}
\newcommand{\boldmu}{{\boldsymbol \mu}}

\newcommand{\nset}[1]{#1_{\{n\}}}
\newcommand{\yref}{y_{\textrm{ref}}}
\newcommand{\cdist}{{D(\nset{x}|x_f)}}
\newcommand{\cdistprime}{{D(\nset{x}|x_{f'})}}
\newcommand{\xfprime}{x_{f'}}

\newcommand{\yE}{{\mathrm E}}
\let\over=\overline


%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Describing the curves: uncertainty propagation and bias correction for predictor effects in simple and generalized linear (mixed) models} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Name1 Surname\textsuperscript{1,2\Yinyang},
Name2 Surname\textsuperscript{2\Yinyang},
Name3 Surname\textsuperscript{2,3\textcurrency},
Name4 Surname\textsuperscript{2},
Name5 Surname\textsuperscript{2\ddag},
Name6 Surname\textsuperscript{2\ddag},
Name7 Surname\textsuperscript{1,2,3*},
with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{2} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{3} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
\ddag These authors also contributed equally to this work.

% Current address notes
\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
\dag Deceased

% Group/Consortium Author Note
\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* correspondingauthor@institute.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}

\fix


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}

\fix

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}

In many applications, outcome predictions or probabilities, are often of interest. Predictions provide a great way to summarize what the regression model is telling us and very useful for interpreting and visualizing model estimates. For example, in logistic regression models, the coefficient estimates are usually not easy to interpret and less informative. Since logistic models are nonlinear (due to the nonlinear link function), in multivariate models with interactions, the magnitude of the effect of change in the outcome depends on the values of the predictor of interest and other predictors. Hence, the conclusions one can make about the estimated effects greatly depends on how well we choose the values of the other variables.  

Both simple and generalized linear (mixed) models (GL(M)Ms) can examine very complex relationships, including nonlinear relationships between response and predictors, interactions between predictors (and via splines for example), and nonlinear transformations via link functions due to their flexibility. This flexibility comes at a cost, for example, complex multivariate models may risk misinterpretation, and miscalculation of quantities of interest. Also, coefficient estimates of models involving nonlinear link functions or interactions lose their direct interpretation \cite{leeper2017interpreting}, meaning that interpretation of derived quantities from these estimates requires some understanding of the specified model. An alternative is to explore the \emph{effects} of predictors on the predictions or probabilities of the outcome at various level of the predictor of interest -- \emph{predictor effects} \cite{fox2009effect, leeper2017package, lenth2018package}. For example, as a way to plan, public health officials may want to know the effects of household income, wealth index, etc, on the predicted probability of having improved water services among slum dwellers.


When visually presented, predictor effects provide a unified and intuitive way of describing relationships from a fitted model, especially complex models involving interaction terms or some kind of transformations on the predictors whose estimates are usually, but not always, a subject to less clarity of interpretation. Further, generating predictor effects together with the associated confidence intervals for regression models has a number of challenges. In particular:
\begin{enumerate}
\item choice of representative values of \emph{focal} predictor(s) and appropriate \emph{model center} for \emph{non-focal} predictors especially in multivariate models
\item  propagation of uncertainty -- can we incorporate uncertainty in nonlinear components of GL(M)Ms? Should we exclude variation in non-focal parameters?
\item bias in the expected mean prediction induced by the nonlinear transformation of the response variable (especially in GL(M)Ms)
\end{enumerate}

The most common way of dealing with the first challenge is taking unique levels of the focal (predictor of interest) predictor if discrete or taking appropriately sized quantiles (or bins) if continuous, and then calculating the predictions while holding non-focal (other predictors other than the focal predictor) predictors at their typical values (e.g., averages) \cite{hanmer2013behind}. This generates -- \emph{predictor effects}\cite{fox2009effect}, \emph{marginal predictions} \cite{leeper2017package} and \emph{estimated marginal means} \cite{lenth2018package}. In this article, we refer to this quantity as predictor effects since it should, for example, tell us what we would expect the presponse to be at a particular value or level of the predictor. Formerly, predictor effects computes the expected outcome by meaningfully holding the non-focal predictors constant (or averaged in some meaningful way) while varying the focal predictor, with the goal that the outcome expected prediction represents how the model responds to the changes in the focal predictor.

The commonly used \proglang{R} software packages (\pkg{emmeans} and \pkg{effects}) for generating predictor effects, by default, averages the non-focal predictors. However, there are a number of choices a number of choices which one has to make when considering this approach -- for example, in the presence of interaction, averaging the interactions (averaging product of interacting predictors) versus product of the averages of the interacting predictors (default for \pkg{emmeans} and \pkg{effects}). We claim that neither of these two approaches is the most appropriate but we show that averaging the interaction closely matches the observed values. To illustrate this, consider models~\ref{eq:simple_inter_higher_no_interaction} and \ref{eq:simple_inter_higher} below, with $x_1$ as the focal predictor:

%
\begin{align}
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \epsilon \label{eq:simple_inter_higher_no_interaction}\\
y &= \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_{23}x_2x_3 + \epsilon \label{eq:simple_inter_higher}
\end{align}
%

We first simulate data with the two models, such that $x_{1,2,3} \sim \mathrm{Normal}(0, 1)$, $\beta_0 = 5$, $\beta_1 = -0.5$, $\beta_2 = 1$, $\beta_3 = 2$ and $\beta_{23} = 5$, and then compare the predictions from the \pkg{emmeans}, \pkg{effects} and our proposed alternative (\pkg{varpred}) to the observed average i.e., $\bar{y}$. Fig~\ref{fig:justify_plots} compares the predictors effects of the two models. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{justify_plots-figure1.pdf}
\caption{{\bf A comparison of \pkg{emmeans}, \pkg{effects} and \pkg{varpred} predictor effects for $x_1$ on $y$ for models with and without interactions.}
The horizontal blue, green and black lines are the mean predictions, i.e., $\bar{\hat{y}}$, the red ones are the mean observed $y$, i.e., $\bar{y}$, while the vertical dotted grey lines are the means of the corresponding focal predictors. The trend lines represents the corresponding $\hat{y}$ at various levels of $x_1$, while holding the other predictors at their average. A: In the absence of interaction, the predicted mean, $\bar{\hat{y}}$, closely matches the observed in all the three approaches. B: Even with the simple interaction between the non-focal predictors, we start seeing larger deviation of the predicted mean, $\bar{\hat{y}}$, from the observed, $\bar{y}$, in two commonly used packages (\pkg{emmeans} and \pkg{effects}), but not, the proposed \pkg{varpred}.}
\label{fig:justify_plots}
\end{figure}

In the absence of interaction (model~\ref{eq:simple_inter_higher_no_interaction}), the three approaches produce similar estimates, which match the observed values, Fig~\ref{fig:justify_plots}A. However, in the presence of interaction, even as simple as the one in model~\ref{eq:simple_inter_higher}, the estimates starts to differ. In particular, \pkg{emmeans} and \pkg{effects} give similar estimates ($\bar{\hat{y}}$) but different from the \pkg{varpred}'s which, however, is similar to the observed average ($\bar{y}$), Fig~\ref{fig:justify_plots}B. To generate Fig~\ref{fig:justify_plots}A, all the three packages averages the non-focal predictors $x_2$ and $x_3$. On other hand, to generate Fig~\ref{fig:justify_plots}B, the difference in the estimates lies on how each of the packages average the interaction term ($x_2x_3$). In particular \pkg{emmeans} and \pkg{effects} computes $\bar{x_2}\bar{x_3}$ while \pkg{varpred} computes $\over{x_2x_3}$.


An alternative to averaging the non-focal predictors, is the \emph{population-based} approach which involves computing the prediction over the population of the non-focal predictors and then averaging across the values of the focal predictor \cite{hanmer2013behind}.

In some applications, we may only be interested in the uncertainties associated with a particular focal predictor -- \emph{isolated} confidence intervals. However, currently, the two packages do not provide straightforward way do achieve this. In addition to prediction lines shown in Fig~\ref{fig:justify_plots}A above, we may want to include the confidence bands associated with the predictions from  all the three methods, as shown in Fig~\ref{fig:justify_ci_plots}. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{justify_ci_plots-figure2.pdf}
\caption{{\bf A comparison of regular and isolated confidence bands.} The description of horizontal, vertical and trend lines remain the same as above. The wider dotted blue curves overlaying the green curves are the regular confidence bands from \pkg{emmeans} and \pkg{effects}, while the narrower black curves crossing at the mean of the focal predictor are the isolated confidence bands from \pkg{varpred}. For simple models, the isolated confidence bands crosses at the model center.}
\label{fig:justify_ci_plots}
\end{figure}

For \pkg{emmeans} and \pkg{effects}, the confidence bands are much wider because they include uncertainties associated with the non-focal predictors, but narrower and crosses at the mean of the focal predictor, i.e., model center, in \pkg{varpred}. In other words, with \pkg{varpred}, we are able to generate isolated confidence bands indicating zero uncertainty at the value of the focal predictor we are more certain about, i.e., mean of the focal predictor. For simple models, the point where the confidence bands crosses is the model center and it corresponds to typical value we choose, in this case, the mean of the focal predictor.


When dealing with nonlinear link functions, the correct predictions, for example, are even much harder to estimate. One approach is to make predictions on the transformed scale (linear predictor scale), and then back-transform to the original scale. However, the back-transformation may either result in biased predictions or requires some approximation. In particular, bias in expected mean prediction induced by nonlinear transformation of the response variable can lead inaccurate predictions.

The main purpose of this article is to discuss and implement various approaches for computing predictor effects and provide an alternative method for computing the associated confidence intervals. We further explore approaches for correcting bias in predictions for GL(M)Ms involving nonlinear link functions.


\section*{Quantities of interest}

Several quantities of interest may be derived from regression models. The first one is the coefficient estimates. Others are \emph{input variables},  \emph{predictors}, \emph{model matrix}, \emph{predicted values} and \emph{marginal effects}. Input variables are the variables that were measured (may be transformed), while predictors are the terms that are entered in the model. Hence, predictors encompass the main effects, but also polynomials of input variables and interaction terms (for example in polynomials and splines) \cite{schielzeth2010simple}. Model matrix refers to the design matrix whose rows include all combination of variables appearing in the interaction terms, along with the typical values of the focal and non-focal predictors.

In simple linear models with no interaction terms, the default output for the coefficient estimates are simple and directly interpretable as the expected change in outcome for a unit change in focal predictor. This is the \emph{unconditional marginal effect} \cite{leeper2017interpreting} and it is constant across all the observations and levels of all other predictors. Consider models ~\ref{eq:simple_inter_higher_no_interaction} and \ref{eq:simple_inter_higher}. The marginal effect of $x_2$ in model~\ref{eq:simple_inter_higher_no_interaction} is $\frac{\partial y}{\partial x_2} = \beta_2$. On the other hand, the marginal effect of $x_2$ in model~\ref{eq:simple_inter_higher} is given by $\frac{\partial y}{\partial x_2} = \beta_2 + \beta_{23}x_3$. In other words, if there are no interactions, the marginal effect of $x_2$ on $y$ is constant, while, if there are interactions in the model, the marginal effect of a change in $x_2$ on $y$ depends on the value of the other \emph{conditioning} predictor $x_3$. As a result, the interpretation of interaction models differs in an important way from linear-additive regression models. 


To distinguish between predictor effects and marginal effects, consider result from a hypothetical  simulated example -- regression of household size as function of household wealth index and age of household head, as shown in Fig~\ref{fig:qoi_age_pred_plot}. Since the model has no interactions, the relationship between the predicted household size and age is linear, hence the marginal effect of age is the slope ($\frac{\Delta \mathrm{hh size}}{\Delta \mathrm{age}}$) of the predictor effect line and can be calculated irrespective of the values of wealth index.

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{qoi_age_pred_plot-figure3.pdf}
\caption{{\bf A comparison of predictor effect and unconditional marginal effect of age on household size.} The black line is the predictor effect trend line line, while the dotted lines indicate the changes along the \code{age} and \code{household size} axis. For a linear model with no interaction, the marginal effect is the slope of the predictor effect line.}
\label{fig:qoi_age_pred_plot}
\end{figure}

Predictor effect, on the other hand, is the expected household size for a particular age, holding wealth index at its mean. In particular, the purpose and goal of a predictor effect seems fairly straightforward; for specified values of a focal predictor, we want to give a point estimate and confidence intervals for the prediction of the model for a typical individual with those values of the predictors.

\section*{Statistical background}

To get an intuition of how conditioning on the mean values of the non-focal predictors work, suppose we are interested in predictor effects of a particular predictor, i.e., focal, $x_f$, from the set of predictors. To keep it simple, assume that the model has no interaction terms. The idea is to \emph{anchor} the values of non-focal predictors to some particular values. For example fixing the values of non-focal predictor(s) at some typical values -- typically determined by averaging (for now) in some meaningful way, for example, arithmetic mean  for continuous and average over the levels of the factors for categorical non-focal predictors. The easiest way to achieve this is by constructing $\bX^\star$ by averaging the columns of non-focal predictors in model matrix $\bX$, and together with appropriately chosen values of focal predictor(s).


Consider a simple linear model with linear predictor $\eta = \bX\bbeta$ and let $g(\boldmu) = \boldeta$ be an identity link function (in the case of simple linear model), where $\boldmu$ is the expected value of response variable $y$. Let $\hat{\bbeta}$ be the estimate of $\bbeta$, together with the estimated covariance matrix $\Sigma = V(\hat{\bbeta})$ of $\hat{\bbeta}$. Let $\mathbf{X^*}$ be the model matrix, inheriting most of its key properties, for example transformations on predictors and interactions from the model matrix, $\mathbf{X}$. Then the prediction $\hat{\boldeta}^\star = \bX^\star\hat{\bbeta}$ is the predictor effect for the focal predictor in question \cite{fox2009effect}.

An alternative, save for later, formulation of predictor effect involves, expressing the linear predictor as the sum of the focal and non-focal predictor linear predictor. In particular, $\eta^\star(x_f, \nset{{\bar{x}}}) = \beta_f x_f + \sum \nset{\beta} \nset{{\bar{x}^\star}}$, where $\nset{{\bar{x}^\star}}$ are the appropriately averaged entries of non-focal predictors and $x_f$ is a vector of values of the focal predictors for a particular observation. 


\subsection*{Dealing with higher order interactions}

Higher order terms such as interactions, splines, polynomials, etc., can be between the non-focal predictors or focal and non-focal predictor(s). In the former case, we treat the interactions as just another column in the variable space (of the model matrix). In the later case, the non-focal variables in the model matrix are averaged as before. However, for the interacting focal predictors, a combination of each unique levels (or quantiles) are first generated and then the interaction terms are generated by multiplying these combinations. For example, consider model~\ref{eq:simple_inter_higher} above. In the first case, if we consider $x_1$ as the focal predictor, the interaction ($x_2x_3$) is between the non-focal predictors, $x_2$ and $x_3$, the linear predictor is

\begin{align*}
\eta^\star(x_{1i}, \nset{{\bar{x}^\star}}) = \beta_0\mathbf{1} + \beta_1 x_{1i} + \beta_2\bar{x}_2 + \beta_3\bar{x}_3 + \beta_{23}\over{x_2x_3}
\end{align*}
where $x_{1i}$ are the carefully chosen levels of the focal predictor. On the other hand, for the second case, if we consider $x_2$ as the focal predictor, then the interaction ($x_2x_3$) is between focal and non-focal predictor. In this case, the linear predictor is given by

\begin{align*}
\eta^\star(x_{2ij}, \nset{{\bar{x}^\star}}) = \beta_0\mathbf{1} + \beta_1 \bar{x}_1 + \beta_2x_{2ij} + \beta_3x_{3j} + \beta_{23}x_{2ij}x_{3j}
\end{align*}

In general, our formulation, even for more complicated interactions, follow these two basic principles -- interaction between non-focal predictors and interaction between focal and non-focal predictors.


\section*{Uncertainty propagation}

What about the confidence intervals (CI)? The limits of the confidence intervals are points, not mean values. In principle, every value of focal predictor has a different CI. The traditional way to compute variances for predictions is $\sigma^2 = \textrm{Diag}(\bX^\star \Sigma \bX^{\star\top})$ \cite{lenth2018package, fox2009effect}, so that the confidence intervals are $\eta \pm q\sigma$, where $q$ is an appropriate quantile of Normal or t distribution. This approach incorporates all the uncertainties -- including the uncertainty due to non-focal predictors.  But what if we are only interested in the uncertainty as a result of the focal predictor, so that the confidence intervals are $\eta \pm q \sigma_f$, i.e., isolated confidence intervals?

Currently, commonly used \proglang{R} packages for constructing predictions do not exclude the uncertainties resulting from the non-focal predictors when computing the CIs. A non-trivial way to exclude uncertainties associated with non-focal predictors in some of these packages is to provide a user defined variance-covariance matrix with the covariances of non-focal terms set to $0$ -- \emph{zeroing-out} variance-covariance matrix. This only works when the input predictors are \emph{centered} prior to model fitting, in case of numerical predictors, and even much complicated when the predictors are categorical. We first describe the variance-covariance based approach and then discuss our proposed method which is based on \emph{centering model matrix} and does not require input predictors to be scaled prior to model fitting.


\subsection*{Variance-covariance}

The computation of $\hat{\eta}^\star$ remains the same as described above. However, to compute $\sigma$, $\Sigma$ is modified by \emph{zeroing-out} (the variance-covariance of all non-focal predictors are set to zero) variances of non-focal terms. Although this is the simplest approach, it requires centering of continuous, i.e., $x_c = x - \bar{x}$, predictors prior to model fitting and proper way to average categorical predictors.

\subsection*{Centered model matrix}

Consider centered model matrix $\bX^{\star}_{c} = \{\bX_f^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$. It follows that the non-focal terms in $\bX^{\star}_{c}$ are all zero in simple models without interactions but are isolated (narrower) around the model center in models involving complex interactions. Consequently the uncertainty due to non-focal predictors are isolated in the computation of $\sigma^2 = \textrm{Diag}(\bX^\star_c \Sigma \bX^{\star\top}_c)$. In addition, the computation of $\bX^{\star}_c$ impacts only on the intercepts and the non-focal terms, i.e., the slopes and variance of the focal predictors are not affected. This means that we can still generate isolated CIs without necessarily centering the predictors prior to model fitting.


\section*{Bias correction}

In many applications, it usually important to report the estimates that reflect the expected values of the untransformed response. However, when dealing with nonlinear link functions, it is even harder to generate correct predictions that reflect the untransformed response due to the bias in the expected mean induced by the nonlinear transformation of the response variable. In such cases, bias correction is needed when back-transforming the predictions to the original scales. Most common approach for bias-adjustment is second-order Taylor approximation \cite{lenth2018package, duursma2003bias}. Here, we describe and implement a different approach, population-based approach for bias correction.


\subsection*{Population-based approach for bias correction}

The most precise (although not necessarily accurate!) way to predict is to condition on values of the focal predictor and make predictions for all observations (members of the population) \cite{hanmer2013behind}. A key point is that the nonlinear transformation involved in these computations is always \emph{one-dimensional}; all of the multivariate computations required are at the stage of collapsing the multidimensional set of predictors for some subset of the population to a one-dimensional distribution of $\eta^\star$.

Once we have got our vector of $\eta^\star$ (which is essentially a set of samples from the distribution over $\eta^\star(x_f, \nset{x})$ for the conditional set), we may want a mean and confidence intervals of the mean on the response (data) scale, i.e. after back-transforming. In other words we can use the observations themselves then we just compute the individual values of $g^{-1}(\eta^\star)$ and compute their mean.

In population-based approach, $\eta^\star$ is constructed as a function of properly constituted levels of focal predictors, $x_f$, and entire population of the non-focal predictors, $\nset{x}$, as opposed to averaged non-focal predictors, $\nset{{\bar{x}}}$, described in the previous section. More specifically:

\begin{itemize}
\item compute linear predictor associated with the non-focal predictors, $\nset{\eta} = \sum \nset{\beta} \nset{x}$
\item compute linear predictor associated with the focal predictors, $\eta_{jf} = \sum{\beta_f x_{jf}}$
\item for every value of the focal predictor, $\eta_{jf}$:

\begin{align}\label{eq:pop_eta} 
\eta_j^\star  &= \eta_{jf} + \nset{\eta} \\
\hat{y}_j  &= \textrm{mean} ~ g^{-1} \left(\eta_j^\star\right)
\end{align}

\end{itemize}
We make similar adjustments to compute the variances of the predictions at every level of the focal predictor:

\begin{align}
\sigma_{jf}^2 = \textrm{Diag}(\bX^\star_{jc} \Sigma \bX^{\star\top}_{jc})
\end{align}
where $\bX^{\star}_{jc} = \{\bX_{jf}^\star, \nset{{\bX}^\star} - \nset{{\bar{\bX}}^\star}\}$ and 

\begin{align}
\mathrm{CI}_j = \mathrm{mean} ~ \eta_j^\star \pm q\sigma_{jf}
\end{align}

For models with random effects components, we make further adjustment to correct for bias induced by the random effects terms. In the population approach, we treat the random effects terms as additional non-focal predictors and simply make adjustment to Equation~\ref{eq:pop_eta}. In particular

\begin{align}\label{eq:pop_eta_re} 
\tau &= \bZ b \nonumber \\
\eta_j^\star  &= \eta_{jf} + \nset{\eta} + \tau_j
\end{align}
where $\bZ$ and $b$ are the design matrix and a vector of random effects, respectively.



\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 

\bibliography{vareffects_plosone}



\end{document}

