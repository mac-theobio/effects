\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx, subfig}
\usepackage{float}
\usepackage{hyperref}

\newcommand{\JD}[1]{{\color{blue} \emph{#1}}}
\newcommand{\bmb}[1]{{\color{RubineRed!70!} \emph{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
\def\code#1{\texttt{#1}}

\setlength{\parindent}{0pt}

\title{Visualizing predictions}

\begin{document}

\maketitle

\section{Introduction}

In this task, we describe various R machineries for displaying predictions in both simple and complex (involving interaction terms) generalized linear models. We first consider existing approaches for constructing model predictions and then describe how our proposed approach is different from the existing ones. When visually presented, predictions provide a unified and intuitive way of describing relationships from a fitted model, especially complex models involving interaction terms or some kind transformations on the dependent variables whose estimates are usually, but not always, a subject to less clarity of interpretation.

A related term to \emph{marginal} predictions is the \emph{conditional} predictions --- it is not clear whether conditional or marginal predictions is more appropriate for a particular research problem \citep{muff2016marginal}. However, we attempt to make the distinction between the two (see \href{https://github.com/mac-theobio/effects/blob/main/glossary.md}{glossary}).

By default, commonly used R packages for constructing predictions do not remove (\emph{zero-out}) the uncertainties resulting from the \emph{non-focal} predictors when computing predictions. A non-trivial way to achieve this in some of these packages is to provide a user defined variance-covariance matrix with the covariances of \emph{non-focal} terms set to $0$ -- \emph{zeroing-out} variance-covariance matrix. This only works when the predictors are centered prior to model fitting, in case of numerical predictors, and even much complicated when the predictors are categorical. We propose a method independent of model scaling to provide more robust \emph{marginal} predictions. 

We can derive various quantities from a fitted regression model. The first and most obvious, is the model coefficient estimates. Others include predicted values of the outcome variable --- 1) predictions at a particular; and 2) mean or median value of the predictors. The first case simply involves evaluation of the fitted model function, say $\hat{f}(X = x)$, at some particular value of $x$. The second case, chooses the values of the predictor based on its distributional properties. 

Most importantly, from these predicted values, we can also generate second class quantities of interest --- \emph{marginal} predictions, which describes the change in the predicted value of the dependent variable after changing one independent variable -- either a discrete change in the categorical variable(s) or an instantaneous change in continuous variables, while all other variable are held at specified values. 

Suppose we are interested in the \emph{marginal} predictions of a particular predictor (hence forth referred as \emph{focal} predictor otherwise \emph{non-focal}), $x_f$, in the set of predictors. To keep it simple, assume that the model has no interaction terms. Then the idea is to fix the values of \emph{non-focal} predictor(s) at some typical values -- typically determined by averaging in some meaningful way, for example, arithmetic mean and average over the levels of the factors of \emph{non-focal} continuous and categorical predictors, respectively. An alternative to \emph{averaging} is \emph{anchoring} which involves picking a fixed value of the predictor. One way to achieve this is using \emph{model matrix},  $\mathbf{X}$, i.e., \emph{anchoring} or \emph{averaging} the columns of \emph{non-focal} terms in $\mathbf{X}$. 


\subsection{Simple linear models}

Consider a simple linear model with linear predictor $\eta = \mathbf{X\beta}$ and let $g(\mu) = \eta$ be an identity link function (in the case of simple linear model), where $\mu$ is the predicted (expected) value of outcome variable  $y$. Let $\hat{\beta}$ be the estimate of $\beta$, together with the estimated covariance matrix $V(\hat{\beta})$ of $\hat{\beta}$. Let the entries of $\mathbf{X^*}$ include all \emph{non-focal} and \emph{focal} predictors. The model matrix, $\mathbf{X^*}$, inherits most of its key properties, for example transformation (e.g., scaling) on the predictors and interactions from the model matrix, $\mathbf{X}$. Then the predicted values $\hat{\eta}^* = \mathbf{X^*}\hat{\beta}$ represents the \emph{marginal} predictions of the focal predictor. Alternatively, we can transform these predictions to response scale using $g^{-1}(\hat{\eta}^*)$.

Further, we can compute the standard errors (SEs) associated with these predictions, $\hat{\eta}^*$, for constructing confidence intervals, as the $\operatorname{sqrt}(\operatorname{diag}(\mathbf{X}^*V(\hat{\beta})\mathbf{X}^{*T}))$. Our proposed method goes one step further of removing the \emph{uncertainty} as a result of \emph{non-focal} predictors are removed. This can be achieved in two ways:
\begin{itemize}
\item using variance-covariance matrix, $V(\hat{\beta})$
\item using centered model matrix, $\mathbf{X}^*$
\end{itemize}

\subsubsection{Variance-covariance}

The computation of $\hat{\eta}^*$ remains the same as described above. However, to compute SEs $V(\hat{\beta})$ is modified by \emph{zeroing-out} (the variance-covariance of all non-focal predictors are assigned zero) entries of \emph{non-focal} terms in $V(\hat{\beta})$. This approach requires \emph{centering} of the predictors in the model matrix, $X$. In other words, the fitted model should have \emph{centered} predictors.


\subsubsection{Centered model matrix}

Suppose the \emph{non-focal} entries in $X^*$ are computed by some kind of averaging or anchoring. Consider centered $X^*$, $X^*_{c} = (X^* - \bar{X^*})$. It follows that the \emph{non-focal} entries in $X^*_{c}$ are all zero. As a result, the SEs are computed as  $\operatorname{sqrt}(\operatorname{diag}(\mathbf{X}^*_{c}V(\hat{\beta})\mathbf{X}^{*T}_{c}))$, which actually \emph{zeros-out}. More generally, \emph{centering}, $\mathbf{X}^*_{c} = \mathbf{X}^* - k$ (for example $k = E(\mathbf{X}^*)$) impacts on the estimated value of the intercept and its associated variance. However, the slopes are not affect by this. The implication of this that since \emph{non-focal} terms in $\mathbf{X}^*_c$ are zero, it doesn't matter what their corresponding values are in the variance-covariance matrix. Hence, we can compute \emph{marginal} predictions from non-centered predictors (in other words, fitted models with predictors in their natural scales).

\subsection{GLMs}


\subsection{LMEs}

\section{Available packages}
The following R packages \pkg{effects}, \pkg{emmeans} and \pkg{margins} implement various schemes for constructing \emph{marginal} predictions. However, currently, their ability to compute \emph{marginal} predictions with zeroed-out non-focal uncertanities is limited to the use of variance-covariance approach which requires the fitted model to be centered. We propose \pkg{varpred} to overcome this limitation.

\section{TODOs}
\begin{enumerate}
\item Models with interactions
\item GLMs
\item LMEs
\end{enumerate}


\section{No interactions}

\subsection{Simulation}

Consider a simple no-interaction terms simulation:
\begin{itemize}
\item $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$ s.t $\epsilon \sim N(0, 1)$ and $\{\beta_0 = -5, \beta_1 = 0.1, \beta_2 = -0.6\}$
\begin{itemize}
\item $x_1 \sim unif(1, 9)$
\item $x_2$ -- two level categorical variable
\end{itemize}
\end{itemize}

<<set_up, echo=FALSE>>=
library(ggplot2)
library(margins)
library(effects)
library(emmeans)
library(jdeffects); jdtheme()

load("predict_plots.rda")

## How to pass arguments??
#library(shellpipes)
#sourceFiles()

@

<<simple_sim, echo=FALSE>>=
set.seed(101)
N <- 100
x1_min <- 1
x1_max <- 9
b0 <- 0.3
b1 <- 0.1
b2 <- -0.6
x2_levels <- factor(c("A", "B"))
df <- expand.grid(x1u = runif(n=N, min=x1_min, max=x1_max)
	, x2u = x2_levels
)
X <- model.matrix(~x1u + x2u, df)
betas <- c(b0, b1, b2)
df$y <- rnorm(nrow(df), mean= X %*% betas, sd=1)
df2 <- df
df <- transform(df
	, x1c = drop(scale(x1u, scale=FALSE)) 
	, x1s = drop(scale(x1u, center=FALSE))
	, x1sd = drop(scale(x1u))
)
head(df)
@

\subsection{Model fitting}

No scaling or centering covariates 

<<model_lm_u>>=
df_temp <- drop(df) 	# margins doesn't work with transform 
                   	# but we need df to extract attributes :(
lm_u <- lm(y ~ x1u + x2u, data = df_temp)
@

Centered covariates ($x - \bar{x}$) model

<<model_lm_c>>=
lm_c <- lm(y ~ x1c + x2u, data = df_temp)
@

Scaled covariates ($x/sd{x}$) model

<<model_lm_s>>=
lm_s <- lm(y ~ x1s + x2u, data = df_temp)
@

Both scaled and centered covariates

<<model_lm_sd>>=
lm_sd <- lm(y ~ x1sd + x2u, data = df_temp)
@
Coefficients estimates

<<model_summary, echo=FALSE>>=
modsummary <- function(patterns="^lm_", fun, simplify = TRUE
	, combine = c("cbind", "rbind"), match_colnames = NULL){
	mod <- objects(name=1L, pattern=patterns, sorted=TRUE)
	combine <- match.arg(combine)
	out <- sapply(mod, function(m){
		out <- fun(get(m))
		out <- round(out, 3)
		if (combine=="rbind"){
			if (!is.null(match_colnames)){
				colnames(out) <- match_colnames
			}
			out <- cbind.data.frame(out, model = m)
		}
		return(out)
	}, simplify = FALSE)
	if (simplify){
		out <- do.call(combine, out)
		if (combine=="cbind") {
			colnames(out) <- mod
		}
	}
	return(out)
}
@

<<simple_coefs>>=
coef_est <- modsummary(fun=coef)
print(coef_est)
@

These four model are equivalent (they all have equal loglikehoods).

<<simple_loglik>>=
ll_est <- modsummary(fun=logLik)
print(ll_est)
@

Variance covariance matrix. It is important to note that the slope variance for unscaled and centered models are the same. This makes it easier to center predictions from uncentered models.
<<simple_vcov>>=
vcov_est <- modsummary(fun=function(x)vcov(x), simplify=TRUE
	, combine="rbind", match_colnames = names(coef(lm_u)))
print(vcov_est)
@

\subsection{All uncertainties included}

<<condional_summary, echo=FALSE>>=
condsummary <- function(mod_list, fun, resp = "y", simplify = TRUE
	, combine = c("cbind", "rbind"), scale_param = list(), fmethod = NULL){
	focal <- names(mod_list)
	combine <- match.arg(combine)
	out <- sapply(focal, function(f){
		mod <- mod_list[[f]]
		out <- fun(mod, f)
		if (inherits(out, c("emmeans", "emmGrid"))) {
			out <- as.data.frame(out)
			oldn <- c(f, "emmean"
				, grep("\\.CL", colnames(out), value=TRUE)
			)
			newn <- c("xvar", "fit", "lwr", "upr")
			colnames(out)[colnames(out) %in% oldn] <-  newn
		} else if (inherits(out, "eff")) {
			out <- as.data.frame(out)
			oldn <- c(f, "lower", "upper")
			newn <- c("xvar", "lwr", "upr")
			colnames(out)[colnames(out) %in% oldn] <-  newn
		} else if (any(colnames(out) %in% c("xvals", "yvals"))) {
			## Not proper way to handle margins object.
			## Maybe extend cplot to return inheritable object
			oldn <- c("xvals", "yvals", "lower", "upper")
			newn <- c("xvar", "fit", "lwr", "upr")
			colnames(out)[colnames(out) %in% oldn] <-  newn
		} else {
			out <- data.frame(out)
			colnames(out)[colnames(out)%in%f] <- "xvar"
		}
		if (combine=="rbind"){
			out <- cbind.data.frame(out, model = f)
			out <- out[, c("xvar", "fit", "lwr", "upr", "model")]
		}
		pp <- names(scale_param)
		if (!is.null(pp)){
			if(grepl("s$",f)){
				vv <- grep("s$",f, value=TRUE)
				xsd <- scale_param[[vv]]
				out[,"xvar"] <- out[,"xvar"]*xsd
			}
			if(grepl("c$",f)){
				vv <- grep("c$",f, value=TRUE)
				xmu <- scale_param[[vv]]
				out[,"xvar"] <- out[,"xvar"] + xmu
			}
			if(grepl("sd$",f)){
				vv <- grep("sd$",pp,value=TRUE)
				xmu <- scale_param[[vv]][1]
				xsd <- scale_param[[vv]][2] 
				out[,"xvar"] <- out[,"xvar"]*xsd + xmu
				
			}
		}
		if (!is.null(fmethod)){
			out[,"method"] <- fmethod
		}
		return(out)
	}, simplify = FALSE)
	f <- attr(out, "focal")
	if (simplify){
		out <- do.call(combine, out)
		if (combine=="cbind") {
			colnames(out) <- mod
		} else {
			rownames(out) <- NULL
		}
	}
	attr(out, "response") <- resp
	return(out)
}
@


<<simple_model, echo=FALSE>>=
simple_models <- list(x1u = lm_u, x1c = lm_c, x1s = lm_s, x1sd = lm_sd)
@


\subsubsection{Continuous predictor}


<<simple_conditional_specific, echo=FALSE, results=hide>>=
## varpred
simple_vpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- varpred(x, f)
}, simplify = TRUE, combine = "rbind", fmethod = "varpred")

## emmeans
simple_empred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	spec <- as.formula(paste0("~", f))
	dd <- emmeans(x, spec=spec, cov.keep=f)
}, simplify = TRUE, combine = "rbind", fmethod = "emmeans")

## effects
simple_efpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- Effect(f, x, xlevels=100)
}, simplify = TRUE, combine = "rbind", fmethod = "effects")

## margins
simple_margpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- cplot(x, f, what="prediction", n=100, draw=FALSE)
}, simplify = TRUE, combine = "rbind", fmethod = "margins")


## Combine all the estimates
simple_pred_all <- do.call("rbind"
	, list(simple_vpred_all, simple_empred_all, simple_efpred_all, simple_margpred_all)
)
@

<<echo=FALSE>>=
class(simple_pred_all) <- c("jdeffects", "data.frame") # plot.effects
simple_pred_all_plot <- (plot(simple_pred_all) 
	+ facet_wrap(~method) 
	+ theme(legend.position="bottom")
)
@

\begin{figure}[H]
\begin{center}
<<simple_pred_all_plot, fig=TRUE, echo=FALSE>>=
simple_pred_all_plot
@
\end{center}
\caption{Predictions on the variable-specific values.}
\label{fig:simple_pred_all_plot}
\end{figure}


\autoref{fig:simple_pred_all_plot}, displays the simple marginal predictions for each of the four models described above, i.e., variable-specific values (original (u), mean centered (c), divided by standard deviation (s), and both mean centered and scaled (sd)). The uncertainties associated with these predictions includes those for non-focal predictors too. The predictions looks different but they are actually similar when plotted on the same scale, see \autoref{fig:simple_pred_spec_plot}.


<<simple_conditional_backtrans, echo=FALSE, results=hide>>=
## Extract scaling parameters
scale_param <- list(x1s = unlist(attributes(df$x1s))
	, x1c = unlist(attributes(df$x1c))
	, x1sd = unlist(attributes(df$x1sd))
)

## varpred
simple_vpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- varpred(x, f, steps=101)
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "varpred"
)

## emmeans
simple_empred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		spec <- as.formula(paste0("~", f))
		dd <- emmeans(x, spec=spec, cov.keep=f)
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "emmeans"
)

## effects
simple_efpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- Effect(f, x, xlevels=100)
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "effects"
)

## margins
simple_margpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- cplot(x, f, what="prediction", n=100, draw=FALSE)
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "margins"
)

## Combine all the estimates
simple_pred_spec <- do.call("rbind"
	, list(simple_vpred_spec, simple_empred_spec, simple_efpred_spec, simple_margpred_spec)
)
@

<<echo=FALSE>>=
class(simple_pred_spec) <- c("jdeffects", "data.frame") # plot.effects
simple_pred_spec_plot <- (plot(simple_pred_spec)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
@

\begin{figure}[H]
\begin{center}
<<simple_pred_spec_plot, fig=TRUE, echo=FALSE>>=
simple_pred_spec_plot
@
\end{center}
\caption{Back-transformed conditional predictions. Remember the model predictors were in different scales -- this implies that if we know the scaling or centering parameter, we can always back-transform model predictions to the original (unscaled) form.}
\label{fig:simple_pred_spec_plot}
\end{figure}

\subsubsection{Categorical predictors}

In this case, it doesn't matter which of the four models we choose since they all give the similar predictions.

<<simple_conditional_specific_cat, echo=FALSE, results=hide>>=
## varpred
simple_models_cat <- list(x2u = lm_u)
simple_vpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- varpred(x, f)
}, simplify = TRUE, combine = "rbind", fmethod = "varpred")

## emmeans
simple_empred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	spec <- as.formula(paste0("~", f))
	dd <- emmeans(x, spec=spec, cov.keep=f)
}, simplify = TRUE, combine = "rbind", fmethod = "emmeans")

## effects
simple_efpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- Effect(f, x, xlevels=100)
}, simplify = TRUE, combine = "rbind", fmethod = "effects")

## margins
simple_margpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- cplot(x, f, what="prediction", n=100, draw=FALSE)
}, simplify = TRUE, combine = "rbind", fmethod = "margins")


## Combine all the estimates
simple_pred_cat <- do.call("rbind"
	, list(simple_vpred_cat, simple_empred_cat, simple_efpred_cat, simple_margpred_cat)
)
@

<<echo=FALSE>>=
class(simple_pred_cat) <- c("jdeffects", "data.frame") # plot.effects
simple_pred_cat_plot <- (plot(simple_pred_cat)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
@

\begin{figure}[H]
\begin{center}
<<simple_pred_cat_plot, fig=TRUE, echo=FALSE>>=
simple_pred_cat_plot
@
\end{center}
\caption{Conditional predictions.}
\end{figure}


\subsection{Zeroed-out uncertainties}

We can remove uncertainties associated with non-focal predictors by \emph{zeroing-out} the variance of non-focal terms of the variance-covariance matrix (possible in all four methods) but this requires that continuous predictors are centered or appropriate contrast is applied in the case of categorical predictors before model fitting. In \pkg{varpred} marginal predictions are computed by centering the model matrix (when computing SEs), whic does not require centered predictors and is independent of contrast specification.

\subsubsection{zeroed-out covariance matrix}

The modification in each of the methods is to either specify \emph{zeroed-out} covariance matrix or a function to compute the variance-covariance matrix as one of the inputs. In particular, in \pkg{varpred}, \pkg{emmeans} and  \pkg{effects} it is specified through \pkg{vcov.} and in \pkg{margins} through \pkg{vcov} argument. However, this seems not to work currently in \pkg{cplot} which computes the predictions in \pkg{margins}.

The \pkg{zero\_vcov} function in \pkg{jdeffects} \emph{zeros-out} variance-covariances of non-focal terms of the predictor in question. For example:

\begin{itemize}
\item full variance-covariance matrix
<<full_vcov, echo=FALSE>>=
lm_u_vcov <- vcov(lm_u)
print(lm_u_vcov)
@

\item \emph{zeroed-out} variance-covariance matrix with \code{x1u} as the focal variable

<<zero_out_ex, echo=FALSE>>=
lm_u_vcov_zero <- zero_vcov(lm_u, focal_vars = "x1u")
print(lm_u_vcov_zero)
@
\end{itemize}


For the continuous predictors, we first compute predictions on predictor-specific scales (\autoref{fig:simple_pred_all_plot_out}) and then back-transform the predictions to original scale (unscale -- uncenter and/or unscale). To clearly distinguish between what happens when apply various scaling schemes, we separately do the plots (see \autoref{fig:marginal_transformed}).

<<simple_marginal_specific, echo=FALSE, results=hide>>=
## varpred
simple_vpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- varpred(x, f, vcov. = zero_vcov(x, f))
}, simplify = TRUE, combine = "rbind", fmethod = "varpred")

## emmeans
simple_empred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	spec <- as.formula(paste0("~", f))
	dd <- emmeans(x, spec=spec, cov.keep=f, vcov. = zero_vcov(x, f))
}, simplify = TRUE, combine = "rbind", fmethod = "emmeans")

## effects
simple_efpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- Effect(f, x, xlevels=100, vcov. = function(x, complete=FALSE)zero_vcov(x, f, complete))
}, simplify = TRUE, combine = "rbind", fmethod = "effects")

## margins
simple_margpred_all <- condsummary(mod_list=simple_models, fun=function(x, f){
	dd <- cplot(x, f, what="prediction", n=100, draw=FALSE, vcov=function(x)zero_vcov(x, f))
}, simplify = TRUE, combine = "rbind", fmethod = "margins")


## Combine all the estimates
simple_pred_all <- do.call("rbind"
	, list(simple_vpred_all, simple_empred_all, simple_efpred_all, simple_margpred_all)
)
@


<<echo=FALSE>>=
class(simple_pred_all) <- c("jdeffects", "data.frame")
simple_pred_all_plot_out <- (plot(simple_pred_all)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
@

\begin{figure}[H]
\begin{center}
<<simple_margpred_all_plot, fig=TRUE, echo=FALSE>>=
simple_pred_all_plot_out
@
\end{center}
\caption{Marginal predictions with the zeroed-out covariances on the variable-specific scale.}
\label{fig:simple_pred_all_plot_out}
\end{figure}




\subsubsection{Centered model matrix}

Marginal predictions in \pkg{effects} and \pkg{emmeans} require models fitted with transformed predictors. However, in \pkg{varpred} we can use model fitted using predictors on their original scale and estimate marginal predictions. In our example, we use the unscaled model \code{lm\_u} and simply set \code{isolate=TRUE}. The predictions (see \autoref{fig:varpred_isolate_num}) are similar to \autoref{fig:marginal_transformed}~b (except \pkg{margins}).

<<simple_marginal_backtrans, echo=FALSE, results=hide>>=
## varpred
simple_vpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- varpred(x, f, vcov. = zero_vcov(x, f))
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "varpred"
)

## emmeans
simple_empred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		spec <- as.formula(paste0("~", f))
		dd <- emmeans(x, spec=spec, cov.keep=f, vcov. = zero_vcov(x, f))
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "emmeans"
)

## effects
simple_efpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- Effect(f, x, xlevels=100, vcov. = function(x, complete=FALSE)zero_vcov(x, f, complete))
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "effects"
)

## margins
simple_margpred_spec <- condsummary(mod_list=simple_models, fun=function(x, f){
		dd <- cplot(x, f, what="prediction", n=100, draw=FALSE, vcov=function(x)zero_vcov(x, f))
	}, simplify = TRUE, combine = "rbind"
	, scale_param = scale_param, fmethod = "margins"
)

## Combine all the estimates
simple_pred_spec <- do.call("rbind"
	, list(simple_vpred_spec, simple_empred_spec, simple_efpred_spec, simple_margpred_spec)
)
@

<<echo=FALSE>>=
## Plot centered and non-centered separately
simple_pred_spec1 <- subset(simple_pred_spec, model=="x1u"|model=="x1s")
simple_pred_spec2 <- subset(simple_pred_spec, model=="x1c"|model=="x1sd")
@

<<label=simple_pred_spec_plot1, fig=TRUE, echo=FALSE, prefix=FALSE, include=FALSE>>=
class(simple_pred_spec1) <- c("jdeffects", "data.frame")
simple_pred_spec_plot1 <- (plot(simple_pred_spec1)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
simple_pred_spec_plot1
@

<<label=simple_pred_spec_plot2, fig=TRUE, echo=FALSE, prefix=FALSE, include=FALSE>>=
class(simple_pred_spec2) <- c("jdeffects", "data.frame")
simple_pred_spec_plot2 <- (plot(simple_pred_spec2)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
simple_pred_spec_plot2
@

\begin{figure}[ht]
\begin{center}
\subfloat[Either unscaled or devided by the \emph{sd}.]{
\includegraphics[width=0.55\textwidth, height=0.54\textheight]{simple_pred_spec_plot1.pdf}
}
~~
\subfloat[Centered or centered and/or devided by \emph{sd}.]{
\includegraphics[width=0.55\textwidth, height=0.54\textheight]{simple_pred_spec_plot2.pdf}
}
\end{center}

\caption{Back-transformed marginal predictions.}
\label{fig:marginal_transformed}
\end{figure}


\begin{figure}[H]
\begin{center}
<<vapred_isolate_numeric, fig=TRUE>>=
## Centered predictions from unscaled model
vpred_c <- varpred(lm_u, focal = "x1u", isolate = TRUE)
plot(vpred_c)
@
\caption{Using \pkg{varpred} to obtain marginal predictions from unscaled model.}
\label{fig:varpred_isolate_num}
\end{center}
\end{figure}


\subsubsection{Categorical predictors}

Back to categorical predictor, \code{x2u}. The default contrast compares the other categories with no-variance reference category (base) and as a result, we can not compute the SEs using \emph{zeroed-out} variance-covariance matrix unless appropriate contrast (\code{"contr.sum"}) was used during model specification. Model matrix centering provides a way to overcome this and computes centered independent of the contrasts. Currently, only \pkg{varpred} provide this functionality.

<<simple_marginal_specific_cat, echo=FALSE, results=hide>>=
## varpred
simple_models_cat <- list(x2u = lm_u)
simple_margvpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- varpred(x, f, isolate=TRUE)
}, simplify = TRUE, combine = "rbind", fmethod = "varpred")

## emmeans
simple_margempred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	spec <- as.formula(paste0("~", f))
	dd <- emmeans(x, spec=spec, cov.keep=f, vcov. = zero_vcov(x, f))
}, simplify = TRUE, combine = "rbind", fmethod = "emmeans")

## effects
simple_margefpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- Effect(f, x, xlevels=100, vcov. = function(x, complete=FALSE)zero_vcov(x, f, complete))
}, simplify = TRUE, combine = "rbind", fmethod = "effects")

## margins
simple_margpred_cat <- condsummary(mod_list=simple_models_cat, fun=function(x, f){
	dd <- cplot(x, f, what="prediction", n=100, draw=FALSE, vcov=function(x)zero_vcov(x, f))
}, simplify = TRUE, combine = "rbind", fmethod = "margins")


## Combine all the estimates
simple_margpred_cat <- do.call("rbind"
	, list(simple_margvpred_cat, simple_margempred_cat, simple_margefpred_cat, simple_margpred_cat)
)
@

<<echo=FALSE>>=
class(simple_margpred_cat) <- c("jdeffects", "data.frame")
simple_margpred_cat_plot <- (plot(simple_margpred_cat)
	+ facet_wrap(~method)
	+ theme(legend.position="bottom")
)
@

\begin{figure}[H]
\begin{center}
<<simple_margpred_cat_plot, fig=TRUE, echo=FALSE>>=
simple_margpred_cat_plot
@
\end{center}
\caption{Conditional predictions.}
\end{figure}


\subsubsection{Comparison with base R predict}

<<compare_baseR, echo=FALSE>>=
@

Compare prediction estimates from \pkg{base R}, \pkg{emmeans} and \pkg{varpred}

<<>>=
## Base R vs emmeans
all.equal(base, emm)
## Base R vs varpred
all.equal(base, jd)
## Implied: emmeans vs varpred
@

\begin{figure}[H]
\begin{center}
<<compare_baseR_plot, fig=TRUE, echo=FALSE>>=
print(plot(1:10))
## print(x1_plot)
@
\end{center}
\caption{Conditional predictions.}
\end{figure}


\clearpage
% References
\bibliographystyle{plainnat}
\bibliography{effects_writeup}

\end{document}
